{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7sKq69CE4c5n",
   "metadata": {
    "id": "7sKq69CE4c5n"
   },
   "source": [
    "# Advanced Graph RAG: Powering Investment Intelligence with Knowledge Graphs\n",
    "\n",
    "*By Saumil Srivastava, AI Engineering Consultant*\n",
    "\n",
    "*<font color=\"#0F172A\">Target Audience: Engineering Leaders, CTOs, Engineering Managers, Product Managers (AI Products)</font>*\n",
    "\n",
    "---\n",
    "\n",
    "## The Challenge: Moving Beyond Siloed Data in Investment Analysis\n",
    "\n",
    "In the world of investment and finance, data is abundant: market prices, company fundamentals, news articles, economic indicators, fund prospectuses. However, this data often resides in disconnected silos. Answering complex, strategic questions can require analysts to manually piece together information from disparate sources, a time-consuming and often incomplete process.\n",
    "\n",
    "**Consider questions like:**\n",
    "* \"Which S&P 500 companies in the 'Technology' sector, with a P/E ratio under 25 and positive recent news sentiment, are also significant holdings in our firm's top-performing ESG-focused ETFs?\"\n",
    "* \"What is the interconnected risk exposure if a major supplier to several companies in our portfolio faces disruption, considering their industries and shared investors?\"\n",
    "* \"Identify emerging competitors to our key holdings based on their technology focus, recent funding, and key personnel hires, even if they are not yet direct market comparables.\"\n",
    "\n",
    "Answering these efficiently and accurately requires understanding not just individual data points, but the *relationships* between them. This is where traditional databases and even simple vector search on text documents fall short.\n",
    "\n",
    "## The Solution: Knowledge Graphs for Connected Insights\n",
    "\n",
    "A **Knowledge Graph (KG)** represents data as a network of entities (like companies, investors, financial instruments, news events) and the rich relationships that connect them. When combined with **Retrieval Augmented Generation (RAG)** powered by Large Language Models (LLMs), this \"Graph RAG\" approach allows us to:\n",
    "\n",
    "1.  **Ask Complex, Multi-Hop Questions:** Traverse multiple layers of relationships to uncover non-obvious connections.\n",
    "2.  **Integrate Diverse Data Types:** Seamlessly link structured financial data, unstructured news text, and inferred insights.\n",
    "3.  **Enhance Context for LLMs:** Provide LLMs with precise, relevant, and interconnected information, leading to more accurate and insightful answers.\n",
    "4.  **Drive Explainable AI:** The graph structure makes it easier to understand *why* an AI system arrived at a particular conclusion or recommendation.\n",
    "\n",
    "This notebook provides a practical walkthrough of building an \"Investment Intelligence KG.\" We'll demonstrate how to construct such a graph from public data, enrich it using LLMs, and then employ advanced Graph RAG techniques to answer sophisticated questions.\n",
    "\n",
    "**For Engineering Leaders & CTOs:** This is about building next-generation data infrastructure for AI that offers a competitive edge in insight generation and operational efficiency.\n",
    "**For Engineering Managers & Product Managers:** This is about equipping your teams with powerful methods to build more intelligent, context-aware applications and features that deliver tangible value.\n",
    "\n",
    "**What We'll Cover:**\n",
    "1.  **Data Foundation:** Using S&P 500 company data, `yfinance` for financial details & news, and a sample ETF list.\n",
    "2.  **Building the Investment KG:** Defining entities (Companies, Stocks, ETFs, News, Sectors, Metrics) and their relationships.\n",
    "3.  **LLM-Powered Graph Enrichment:** Inferring sentiment from news, generating company risk/opportunity summaries.\n",
    "4.  **Advanced Graph RAG:**\n",
    "    * **Text2Cypher:** LLMs generating Neo4j Cypher queries from natural language.\n",
    "    * **Vector Search on Graph Data:** Finding semantically similar financial entities or concepts.\n",
    "5.  **Strategic Advantage:** Understanding when Graph RAG is superior to simpler vector RAG for financial use cases.\n",
    "\n",
    "Let's begin by setting up our environment and loading the foundational data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_lbVve4v9bYw",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 0. Install necessary libraries ---\n",
    "# Using versions known to be compatible for stability\n",
    "\n",
    "%pip install langchain==0.1.17 neo4j==5.20.0 openai tiktoken==0.7.0 json-repair==0.39.1 yfinance pyvis requests-ratelimiter==0.4.2 langchain-openai datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vS7J0XrRAm-h",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downgrade google-genai to the last release that accepted httpx 0.27.x\n",
    "%pip install --upgrade \"google-genai<1.15.0\"  # 1.14.0 as of today\n",
    "%pip install --force-reinstall \"httpx==0.27.2\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "TD7Cqp1r4c5s",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from typing import Optional, List, Dict, Any\n",
    "from getpass import getpass\n",
    "from io import StringIO # To read string data as CSV\n",
    "import time\n",
    "import yfinance as yf # For fetching stock data and news\n",
    "import ast # For safely evaluating string-formatted lists\n",
    "\n",
    "from google.colab import userdata\n",
    "\n",
    "from openai import OpenAI\n",
    "from neo4j import GraphDatabase, basic_auth\n",
    "\n",
    "from requests_ratelimiter import LimiterSession, RequestRate, Limiter, Duration # Added for yfinance rate limiting\n",
    "import ast # For safely evaluating string-formatted lists\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "import logging\n",
    "logger = logging.getLogger('my_logger')\n",
    "\n",
    "logging.basicConfig()\n",
    "\n",
    "\n",
    "print(\"--- Library Installation Complete ---\")\n",
    "\n",
    "# --- 1. Configure OpenAI API Key ---\n",
    "OPENAI_API_KEY = \"\"\n",
    "if os.getenv(\"OPENAI_API_KEY\"):\n",
    "    OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "else:\n",
    "    try:\n",
    "        OPENAI_API_KEY = userdata.get(\"OPENAI_API_KEY\") # Using a specific key name\n",
    "    except userdata.SecretNotFoundError:\n",
    "        print(\"OpenAI API Key 'OPENAI_API_KEY_MAVEN' not found in Colab Secrets.\")\n",
    "        OPENAI_API_KEY = getpass(\"Please enter your OpenAI API Key: \")\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
    "\n",
    "if not OPENAI_API_KEY.startswith(\"sk-\"):\n",
    "    print(\"⚠️ OpenAI API Key does not look valid. Please check.\")\n",
    "else:\n",
    "    print(\"✅ OpenAI API key configured.\")\n",
    "\n",
    "# --- 2. Configure Neo4j Credentials ---\n",
    "NEO4J_URI = \"\"\n",
    "NEO4J_USERNAME = \"neo4j\"\n",
    "NEO4J_PASSWORD = \"\"\n",
    "\n",
    "try:\n",
    "    NEO4J_URI = userdata.get('NEO4J_URI') # Using a specific key name\n",
    "    NEO4J_PASSWORD = userdata.get('NEO4J_PASSWORD') # Using a specific key name\n",
    "    if NEO4J_URI and NEO4J_PASSWORD:\n",
    "         print(\"✅ Neo4j credentials loaded from Colab Secrets.\")\n",
    "    else:\n",
    "        raise userdata.SecretNotFoundError\n",
    "except userdata.SecretNotFoundError:\n",
    "    print(\"Neo4j credentials ('NEO4J_URI_MAVEN', 'NEO4J_PASSWORD_MAVEN') not found or incomplete in Colab Secrets.\")\n",
    "    NEO4J_URI = input(\"Enter your Neo4j URI (e.g., neo4j+s://your-instance.databases.neo4j.io or bolt://localhost:7687): \")\n",
    "    NEO4J_PASSWORD = getpass(\"Neo4j Password: \")\n",
    "\n",
    "neo4j_driver = None\n",
    "if NEO4J_URI and NEO4J_PASSWORD:\n",
    "    try:\n",
    "        neo4j_driver = GraphDatabase.driver(NEO4J_URI, auth=basic_auth(NEO4J_USERNAME, NEO4J_PASSWORD))\n",
    "        neo4j_driver.verify_connectivity()\n",
    "        print(\"✅ Connected to Neo4j database.\")\n",
    "    except Exception as e:\n",
    "        print(f\"🛑 Error connecting to Neo4j: {e}\")\n",
    "else:\n",
    "    print(\"🛑 Neo4j URI or Password not provided. Please set them to continue.\")\n",
    "\n",
    "\n",
    "# --- 3. Initialize OpenAI Client ---\n",
    "openai_client = OpenAI()\n",
    "\n",
    "print(\"\\n--- Setup Complete ---\")\n",
    "print(f\"Neo4j Driver Initialized: {True if neo4j_driver else False}\")\n",
    "print(f\"OpenAI Client Initialized: {True if openai_client else False}\")\n",
    "\n",
    "print(\"\\n--- Setup Complete ---\")\n",
    "print(f\"Neo4j Driver Initialized: {True if neo4j_driver else False}\")\n",
    "print(f\"OpenAI Client Initialized: {True if openai_client else False}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "LNp0GpAf4c5v",
   "metadata": {
    "id": "LNp0GpAf4c5v"
   },
   "source": [
    "## Part 1: Data Foundation - S&P 500 Companies, `yfinance`, & Sample ETFs\n",
    "\n",
    "Our Investment Intelligence KG will be built from three main sources for this demonstration:\n",
    "\n",
    "1.  **S&P 500 Company List:** A foundational list of large-cap U.S. companies, their tickers, names, and sectors.\n",
    "    * *Source Example for full dataset:* You can find S&P 500 component lists on Kaggle (e.g., search \"S&P 500 companies list csv\") or other financial data sites.\n",
    "2.  **`yfinance` Library:** We'll use this popular Python library to fetch:\n",
    "    * Detailed company information (like business summaries, market cap, P/E ratios) for a *subset* of S&P 500 companies.\n",
    "    * Recent news headlines related to these companies.\n",
    "3.  **Sample ETF Data:** A small, manually defined list of ETFs, their investment focus, and some sample holdings (using tickers from our S&P 500 list). In a real system, this would come from a dedicated financial data provider.\n",
    "\n",
    "**Focus on Practicality:** We'll use a small, embedded sample of S&P 500 companies and ETFs directly in the notebook to ensure it runs smoothly for everyone. The `yfinance` calls will be limited to a few companies to manage execution time and API call politeness. This approach balances demonstrating powerful techniques with ease of use for a learning environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "J4NoeGUmpTVN",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gX3QN7pWgBI5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# \"\"\"\n",
    "# --------------------------------------------------------------------------------------\n",
    "#  Loading Data from Hugging Face Hub\n",
    "# --------------------------------------------------------------------------------------\n",
    "logging.info(\"\\n--- Attempting to Load Foundational Data from Hugging Face Hub ---\")\n",
    "\n",
    "\n",
    "YOUR_HF_USERNAME = \"s4um1l\" #\n",
    "HF_COMPANIES_DATASET_NAME = f\"{YOUR_HF_USERNAME}/investment-kg-companies\"\n",
    "HF_INVESTORS_DATASET_NAME = f\"{YOUR_HF_USERNAME}/investment-kg-investors\"\n",
    "HF_INVESTMENTS_DATASET_NAME = f\"{YOUR_HF_USERNAME}/investment-kg-investments\"\n",
    "HF_ETFS_DATASET_NAME = f\"{YOUR_HF_USERNAME}/investment-kg-etfs\"\n",
    "\n",
    "LOAD_FROM_HF = True # Set to True once you have pushed your datasets to the Hub\n",
    "\n",
    "if LOAD_FROM_HF and YOUR_HF_USERNAME != \"your-hf-username\":\n",
    "    try:\n",
    "        logging.info(f\"Loading datasets from Hugging Face Hub user: {YOUR_HF_USERNAME}...\")\n",
    "\n",
    "        companies_hf = load_dataset(HF_COMPANIES_DATASET_NAME)\n",
    "        detailed_companies_data = list(companies_hf[\"train\"])\n",
    "\n",
    "        investors_hf = load_dataset(HF_INVESTORS_DATASET_NAME)\n",
    "        investor_df = investors_hf[\"train\"].to_pandas()\n",
    "\n",
    "        investments_hf = load_dataset(HF_INVESTMENTS_DATASET_NAME)\n",
    "        investment_relationships_df = investments_hf[\"train\"].to_pandas()\n",
    "\n",
    "        etfs_hf = load_dataset(HF_ETFS_DATASET_NAME)\n",
    "        etf_df = etfs_hf[\"train\"].to_pandas()\n",
    "\n",
    "        # Post-processing for loaded data (e.g., JSON parsing, datetime conversion)\n",
    "        for company_data_item in detailed_companies_data:\n",
    "            if 'news' in company_data_item and isinstance(company_data_item['news'], str):\n",
    "                try:\n",
    "                    company_data_item['news'] = json.loads(company_data_item['news'])\n",
    "                    for news_item in company_data_item.get(\"news\", []):\n",
    "                        if news_item.get(\"publishTime\") and isinstance(news_item.get(\"publishTime\"), str):\n",
    "                            try:\n",
    "                                news_item[\"publishTime\"] = pd.to_datetime(news_item[\"publishTime\"])\n",
    "                            except Exception as e:\n",
    "                                logging.warning(f\"Could not parse publishTime '{news_item['publishTime']}' from HF data: {e}\")\n",
    "                                news_item[\"publishTime\"] = None\n",
    "                except json.JSONDecodeError:\n",
    "                    logging.warning(f\"Could not parse news JSON for {company_data_item.get('ticker')}\")\n",
    "                    company_data_item['news'] = []\n",
    "\n",
    "        logging.info(\"✅ Data successfully loaded from Hugging Face Hub.\")\n",
    "        # Set sp500_df_base from the loaded company data for consistency if needed by other cells\n",
    "        if detailed_companies_data:\n",
    "            sp500_df_base = pd.DataFrame([\n",
    "                {\"Symbol\": c.get(\"ticker\"), \"Name\": c.get(\"name\"), \"Sector\": c.get(\"sector\")}\n",
    "                for c in detailed_companies_data\n",
    "            ])\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"🛑 Failed to load data from Hugging Face Hub: {e}\")\n",
    "        logging.warning(\"Falling back to using the embedded sample data defined in this cell.\")\n",
    "        LOAD_FROM_HF = False\n",
    "\n",
    "# if not LOAD_FROM_HF:\n",
    "#     logging.info(\"Using embedded sample data as LOAD_FROM_HF is False or loading failed.\")\n",
    "#     detailed_companies_data = detailed_companies_data_for_hf\n",
    "#     investor_df = investor_df_for_hf\n",
    "#     investment_relationships_df = investment_relationships_df_for_hf\n",
    "#     etf_df = etf_df_for_hf\n",
    "#     # Convert publishTime for embedded news (which were stored as strings for HF prep)\n",
    "#     # Also, convert news JSON string back to list of dicts for detailed_companies_data\n",
    "#     for company in detailed_companies_data:\n",
    "#         if 'news' in company and isinstance(company['news'], str):\n",
    "#             try:\n",
    "#                 company['news'] = json.loads(company['news'])\n",
    "#             except json.JSONDecodeError:\n",
    "#                 logging.warning(f\"Could not parse embedded news JSON for {company.get('ticker')}\")\n",
    "#                 company['news'] = []\n",
    "\n",
    "#         for news_item in company.get(\"news\", []):\n",
    "#             if isinstance(news_item.get(\"publishTime\"), str):\n",
    "#                 try: news_item[\"publishTime\"] = pd.to_datetime(news_item[\"publishTime\"])\n",
    "#                 except Exception as e:\n",
    "#                     logging.warning(f\"Could not parse publishTime '{news_item['publishTime']}' for {company['ticker']} (fallback): {e}\")\n",
    "#                     news_item[\"publishTime\"] = None\n",
    "\n",
    "\n",
    "logging.info(\"\\n--- Foundational Data Ready for KG Population ---\")\n",
    "\n",
    "# --- Optional: Display sample of the prepared data ---\n",
    "if detailed_companies_data:\n",
    "    logging.info(\"\\nSample of prepared detailed_companies_data (first company):\")\n",
    "    temp_print_data = detailed_companies_data[0].copy()\n",
    "    if 'news' in temp_print_data and temp_print_data['news'] is not None:\n",
    "        # Ensure news is a list of dicts before trying to process publishTime\n",
    "        if isinstance(temp_print_data['news'], list):\n",
    "            temp_print_data['news'] = [news_item.copy() for news_item in temp_print_data['news']]\n",
    "            for news_item in temp_print_data['news']:\n",
    "                if isinstance(news_item.get('publishTime'), pd.Timestamp):\n",
    "                    news_item['publishTime'] = news_item['publishTime'].isoformat()\n",
    "                elif news_item.get('publishTime') is None:\n",
    "                     news_item['publishTime'] = 'N/A'\n",
    "        elif isinstance(temp_print_data['news'], str): # If it's still a JSON string (e.g. fallback path didn't re-parse)\n",
    "            try:\n",
    "                parsed_news = json.loads(temp_print_data['news'])\n",
    "                temp_print_data['news'] = [news_item.copy() for news_item in parsed_news]\n",
    "                for news_item in temp_print_data['news']: # Process again after parsing\n",
    "                    if isinstance(news_item.get('publishTime'), pd.Timestamp):\n",
    "                        news_item['publishTime'] = news_item['publishTime'].isoformat()\n",
    "                    elif news_item.get('publishTime') is None:\n",
    "                        news_item['publishTime'] = 'N/A'\n",
    "            except json.JSONDecodeError:\n",
    "                 logging.warning(f\"Could not parse news for printing for {temp_print_data.get('ticker')}\")\n",
    "\n",
    "\n",
    "    print(json.dumps(temp_print_data, indent=2))\n",
    "\n",
    "if not investor_df.empty:\n",
    "    logging.info(\"\\nSample of prepared investor_df:\")\n",
    "    print(investor_df.head().to_string())\n",
    "\n",
    "if not investment_relationships_df.empty:\n",
    "    logging.info(\"\\nSample of prepared investment_relationships_df:\")\n",
    "    print(investment_relationships_df.head().to_string())\n",
    "\n",
    "if not etf_df.empty:\n",
    "    logging.info(\"\\nSample of prepared etf_df:\")\n",
    "    print(etf_df[['ticker', 'name', 'investment_focus', 'asset_class', 'expenseRatio', 'AUM', 'top_holdings_tickers']].head().to_string())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vlD5QGOfSl7_",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from pyvis.network import Network\n",
    "from IPython.display import HTML, display\n",
    "import logging\n",
    "\n",
    "# Ensure logging is configured\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Ensure detailed_companies_data and etf_df are available from Cell 4\n",
    "if 'detailed_companies_data' not in locals() or 'etf_df' not in locals():\n",
    "    logging.error(\"🛑 Foundational data (detailed_companies_data or etf_df) not loaded. Please run Cell 4 first.\")\n",
    "else:\n",
    "    logging.info(\"\\n--- Building In-Memory Graph with NetworkX ---\")\n",
    "\n",
    "    G = nx.DiGraph() # Directed graph\n",
    "\n",
    "    # Define colors and sizes for node types for Pyvis\n",
    "    node_colors = {\n",
    "        \"Company\": \"skyblue\",\n",
    "        \"ETF\": \"lightgreen\",\n",
    "        \"NewsArticle\": \"lightcoral\",\n",
    "        \"Sector\": \"gold\",\n",
    "        \"Industry\": \"khaki\",\n",
    "        \"AssetClass\": \"lightpink\"\n",
    "    }\n",
    "    node_sizes = {\n",
    "        \"Company\": 25,\n",
    "        \"ETF\": 20,\n",
    "        \"NewsArticle\": 15,\n",
    "        \"Sector\": 18,\n",
    "        \"Industry\": 18,\n",
    "        \"AssetClass\": 18\n",
    "    }\n",
    "\n",
    "    # To keep the visualization manageable for the demo, let's process only a subset\n",
    "    # For example, the first 2-3 companies and related ETFs.\n",
    "    # We'll use all companies from detailed_companies_data as it's already small.\n",
    "\n",
    "    processed_nodes = set()\n",
    "\n",
    "    # Add Company nodes and their related Sector, Industry, NewsArticle nodes\n",
    "    for company_info in detailed_companies_data:\n",
    "        if company_info.get(\"error\"):\n",
    "            logging.warning(f\"Skipping company {company_info.get('ticker')} due to previous fetch error.\")\n",
    "            continue\n",
    "\n",
    "        company_ticker = company_info[\"ticker\"]\n",
    "        company_name = company_info.get(\"name\", company_ticker)\n",
    "\n",
    "        if company_ticker not in processed_nodes:\n",
    "            G.add_node(\n",
    "                company_ticker,\n",
    "                label=company_name[:30], # Pyvis label\n",
    "                title=f\"Ticker: {company_ticker}\\nName: {company_name}\\nSector: {company_info.get('sector', 'N/A')}\\nIndustry: {company_info.get('industry', 'N/A')}\\nSummary: {company_info.get('summary', 'N/A')[:100]}...\", # Pyvis hover title\n",
    "                entity_type=\"Company\",\n",
    "                color=node_colors[\"Company\"],\n",
    "                size=node_sizes[\"Company\"],\n",
    "                sector=company_info.get(\"sector\"),\n",
    "                industry=company_info.get(\"industry\"),\n",
    "                summary=company_info.get(\"summary\")\n",
    "            )\n",
    "            processed_nodes.add(company_ticker)\n",
    "\n",
    "        # Sector node and relationship\n",
    "        sector_name = company_info.get(\"sector\")\n",
    "        if sector_name and sector_name != \"N/A\":\n",
    "            if sector_name not in processed_nodes:\n",
    "                G.add_node(sector_name, label=sector_name, title=f\"Sector: {sector_name}\", entity_type=\"Sector\", color=node_colors[\"Sector\"], size=node_sizes[\"Sector\"])\n",
    "                processed_nodes.add(sector_name)\n",
    "            G.add_edge(company_ticker, sector_name, title=\"OPERATES_IN_SECTOR\")\n",
    "\n",
    "        # Industry node and relationship\n",
    "        industry_name = company_info.get(\"industry\")\n",
    "        if industry_name and industry_name != \"N/A\":\n",
    "            if industry_name not in processed_nodes:\n",
    "                G.add_node(industry_name, label=industry_name, title=f\"Industry: {industry_name}\", entity_type=\"Industry\", color=node_colors[\"Industry\"], size=node_sizes[\"Industry\"])\n",
    "                processed_nodes.add(industry_name)\n",
    "            G.add_edge(company_ticker, industry_name, title=\"OPERATES_IN_INDUSTRY\")\n",
    "\n",
    "        # NewsArticle nodes and relationships\n",
    "        for news_item in company_info.get(\"news\", []):\n",
    "            news_title = news_item.get(\"title\")\n",
    "            news_link = news_item.get(\"link\") # Use link as a more unique ID if titles can repeat\n",
    "\n",
    "            if news_title and news_link: # Ensure there's a title and link\n",
    "                news_node_id = news_link # Use link as node ID for uniqueness\n",
    "                if news_node_id not in processed_nodes:\n",
    "                    G.add_node(\n",
    "                        news_node_id,\n",
    "                        label=news_title[:40] + \"...\", # Pyvis label\n",
    "                        title=f\"Title: {news_title}\\nPublisher: {news_item.get('publisher', 'N/A')}\\nLink: {news_link}\", # Pyvis hover title\n",
    "                        entity_type=\"NewsArticle\",\n",
    "                        color=node_colors[\"NewsArticle\"],\n",
    "                        size=node_sizes[\"NewsArticle\"],\n",
    "                        publisher=news_item.get(\"publisher\"),\n",
    "                        publishTime=str(news_item.get(\"publishTime\")) # Convert datetime to string for attributes\n",
    "                    )\n",
    "                    processed_nodes.add(news_node_id)\n",
    "                G.add_edge(company_ticker, news_node_id, title=\"MENTIONED_IN_NEWS\")\n",
    "\n",
    "    # Add ETF nodes and their related AssetClass and Company (holdings) nodes\n",
    "    if not etf_df.empty:\n",
    "        for _, etf_row in etf_df.iterrows():\n",
    "            if pd.notna(etf_row.get(\"error\")): # Skip if ETF fetch had an error\n",
    "                logging.warning(f\"Skipping ETF {etf_row.get('ticker')} due to previous fetch error.\")\n",
    "                continue\n",
    "\n",
    "            etf_ticker = etf_row[\"ticker\"]\n",
    "            etf_name = etf_row.get(\"name\", etf_ticker)\n",
    "\n",
    "            if etf_ticker not in processed_nodes:\n",
    "                G.add_node(\n",
    "                    etf_ticker,\n",
    "                    label=etf_name[:30],\n",
    "                    title=f\"Ticker: {etf_ticker}\\nName: {etf_name}\\nFocus: {etf_row.get('investment_focus', 'N/A')[:100]}...\",\n",
    "                    entity_type=\"ETF\",\n",
    "                    color=node_colors[\"ETF\"],\n",
    "                    size=node_sizes[\"ETF\"],\n",
    "                    investment_focus=etf_row.get(\"investment_focus\"),\n",
    "                    asset_class_name=etf_row.get(\"asset_class\") # Store asset class name for linking\n",
    "                )\n",
    "                processed_nodes.add(etf_ticker)\n",
    "\n",
    "            # AssetClass node and relationship\n",
    "            asset_class_name = etf_row.get(\"asset_class\")\n",
    "            if asset_class_name and asset_class_name != \"N/A\":\n",
    "                if asset_class_name not in processed_nodes:\n",
    "                    G.add_node(asset_class_name, label=asset_class_name, title=f\"Asset Class: {asset_class_name}\", entity_type=\"AssetClass\", color=node_colors[\"AssetClass\"], size=node_sizes[\"AssetClass\"])\n",
    "                    processed_nodes.add(asset_class_name)\n",
    "                G.add_edge(etf_ticker, asset_class_name, title=\"FOCUSES_ON_ASSET_CLASS\")\n",
    "\n",
    "            # ETF_HOLDS_STOCK relationships\n",
    "            for holding_ticker in etf_row.get(\"top_holdings_tickers\", []):\n",
    "                if G.has_node(holding_ticker) and G.has_node(etf_ticker): # Ensure both nodes exist\n",
    "                    G.add_edge(etf_ticker, holding_ticker, title=\"HOLDS_STOCK\")\n",
    "                else:\n",
    "                    logging.warning(f\"Could not create HOLDS_STOCK edge for ETF {etf_ticker} to {holding_ticker} (one or both nodes missing in graph).\")\n",
    "    else:\n",
    "        logging.warning(\"ETF DataFrame (etf_df) is empty. No ETF nodes or relationships will be added to NetworkX graph.\")\n",
    "\n",
    "\n",
    "    logging.info(f\"✅ NetworkX graph built with {G.number_of_nodes()} nodes and {G.number_of_edges()} edges.\")\n",
    "\n",
    "    # --- Visualize with Pyvis ---\n",
    "    if G.number_of_nodes() > 0:\n",
    "        logging.info(\"\\n--- Generating Interactive Visualization with Pyvis (this might take a moment) ---\")\n",
    "\n",
    "        # Create a Pyvis network\n",
    "        nt = Network(notebook=True, height=\"750px\", width=\"100%\", cdn_resources='in_line', directed=True)\n",
    "\n",
    "        # Add nodes and edges from NetworkX graph to Pyvis network\n",
    "        for node, attrs in G.nodes(data=True):\n",
    "            nt.add_node(\n",
    "                node,\n",
    "                label=attrs.get('label', str(node)),\n",
    "                title=attrs.get('title', str(node)),\n",
    "                color=attrs.get('color', 'grey'),\n",
    "                size=attrs.get('size', 15),\n",
    "                shape='dot' # Default shape, can be customized further based on entity_type\n",
    "            )\n",
    "\n",
    "        for source, target, attrs in G.edges(data=True):\n",
    "            nt.add_edge(source, target, title=attrs.get('title', 'RELATED_TO'))\n",
    "        nt.toggle_physics(True)  # Enable force-directed algorithm\n",
    "\n",
    "        # Configure physics and interaction for better layout and usability\n",
    "        nt.set_options(\"\"\"\n",
    "        var options = {\n",
    "          \"physics\": {\n",
    "            \"barnesHut\": {\n",
    "              \"gravitationalConstant\": -2000,\n",
    "              \"centralGravity\": 0.2,\n",
    "              \"springLength\": 120,\n",
    "              \"springConstant\": 0.02,\n",
    "              \"damping\": 0.09,\n",
    "              \"avoidOverlap\": 0.1\n",
    "            },\n",
    "            \"minVelocity\": 0.75,\n",
    "            \"stabilization\": {\n",
    "                \"enabled\": true,\n",
    "                \"iterations\": 200,\n",
    "                \"fit\": true\n",
    "            }\n",
    "          },\n",
    "          \"interaction\": {\n",
    "            \"hover\": true,\n",
    "            \"tooltipDelay\": 200,\n",
    "            \"hideEdgesOnDrag\": true,\n",
    "            \"hideNodesOnDrag\": false\n",
    "          },\n",
    "          \"nodes\": {\n",
    "            \"font\": {\n",
    "              \"size\": 12,\n",
    "              \"face\": \"Tahoma\"\n",
    "            }\n",
    "          },\n",
    "          \"edges\": {\n",
    "            \"arrows\": {\n",
    "              \"to\": { \"enabled\": true, \"scaleFactor\": 0.7 }\n",
    "            },\n",
    "            \"color\": {\n",
    "              \"inherit\": \"from\"\n",
    "            },\n",
    "            \"smooth\": {\n",
    "              \"type\": \"continuous\",\n",
    "              \"roundness\": 0.2\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "        \"\"\")\n",
    "        nt.from_nx(G)\n",
    "        # Save and show the graph\n",
    "        try:\n",
    "            file_name = \"investment_kg_visualization.html\"\n",
    "            nt.save_graph(file_name)\n",
    "            logging.info(f\"✅ Pyvis graph saved to {file_name}\")\n",
    "\n",
    "            # Display in Colab (if in Colab environment)\n",
    "            # For local Jupyter, nt.show(file_name) would open in a browser.\n",
    "            # In Colab, we often display the HTML directly or provide a link.\n",
    "            HTML(f'<p>Interactive graph saved as <a href=\"{file_name}\" target=\"_blank\">{file_name}</a>. Open it in a new tab for best experience.</p> <iframe src=\"{file_name}\" width=\"100%\" height=\"750px\" style=\"border: 1px solid lightgrey;\"></iframe>')\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"🛑 Error generating or displaying Pyvis graph: {e}\")\n",
    "            logging.info(\"Pyvis visualization might not display correctly in all environments directly. Try opening the generated HTML file.\")\n",
    "    else:\n",
    "        logging.warning(\"⚠️ NetworkX graph is empty. Skipping Pyvis visualization.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZBp5D31rTaWs",
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "-mEArpM84c5y",
   "metadata": {
    "id": "-mEArpM84c5y"
   },
   "source": [
    "## Part 2: Building the Investment Knowledge Graph\n",
    "\n",
    "With our foundational data loaded, we'll now construct the Neo4j Knowledge Graph. This involves creating nodes for each entity type and establishing meaningful relationships between them.\n",
    "\n",
    "**Entities (Nodes) for our Investment KG:**\n",
    "* `:Company` (Symbol (ticker), Name, Sector, Industry, Summary, MarketCap, P/E, Beta, Website)\n",
    "* `:Stock` (Symbol/Ticker - often merged with Company or a distinct node linked to it)\n",
    "* `:ETF` (Ticker, Name, InvestmentFocus, AssetClass)\n",
    "* `:NewsArticle` (Title, Publisher, PublishTime, Link, InferredSentiment)\n",
    "* `:Sector` (Name)\n",
    "* `:Industry` (Name)\n",
    "* `__Entity__` (Generic label for all nodes for unified indexing)\n",
    "\n",
    "**Relationships:**\n",
    "* `OPERATES_IN_SECTOR` (Company -> Sector)\n",
    "* `OPERATES_IN_INDUSTRY` (Company -> Industry)\n",
    "* `HAS_STOCK_DATA` (Company -> Stock, if Stock is a separate node, or properties on Company)\n",
    "* `MENTIONED_IN_NEWS` (Company -> NewsArticle)\n",
    "* `HAS_SENTIMENT` (NewsArticle -> SentimentNode, or as a property on NewsArticle)\n",
    "* `ETF_HOLDS_STOCK` (ETF -> Company/Stock)\n",
    "* `FOCUSES_ON_ASSET_CLASS` (ETF -> AssetClassNode)\n",
    "\n",
    "**Strategic Importance of This Structure:**\n",
    "This graph structure allows you to model the complex interplay between companies, their financial performance, market news, and investment vehicles like ETFs. It enables queries that can trace the impact of news on specific stocks, analyze ETF compositions based on company fundamentals, or identify companies within specific sectors that meet complex criteria. This interconnected view is often crucial for sophisticated investment analysis and is a core strength that graph databases bring to AI engineering in finance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "maJUqqCx4c5z",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import pandas as pd # Ensure pandas is imported if not already\n",
    "from typing import List, Dict, Any # Ensure these are imported\n",
    "\n",
    "# Ensure logging is configured\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "def create_investment_kg_constraints_updated(driver):\n",
    "    \"\"\"Creates unique constraints on node properties for data integrity and performance.\"\"\"\n",
    "    if not driver:\n",
    "        logging.error(\"Neo4j driver not available. Skipping constraint creation.\")\n",
    "        return\n",
    "    try:\n",
    "        with driver.session(database=\"neo4j\") as session: # Ensure correct database\n",
    "            constraints = [\n",
    "                \"CREATE CONSTRAINT IF NOT EXISTS FOR (c:Company) REQUIRE c.ticker IS UNIQUE\",\n",
    "                \"CREATE CONSTRAINT IF NOT EXISTS FOR (s:Stock) REQUIRE s.ticker IS UNIQUE\", # If Stock is a separate node\n",
    "                \"CREATE CONSTRAINT IF NOT EXISTS FOR (e:ETF) REQUIRE e.ticker IS UNIQUE\",\n",
    "                \"CREATE CONSTRAINT IF NOT EXISTS FOR (n:NewsArticle) REQUIRE n.link IS UNIQUE\",\n",
    "                \"CREATE CONSTRAINT IF NOT EXISTS FOR (sec:Sector) REQUIRE sec.name IS UNIQUE\",\n",
    "                \"CREATE CONSTRAINT IF NOT EXISTS FOR (ind:Industry) REQUIRE ind.name IS UNIQUE\",\n",
    "                \"CREATE CONSTRAINT IF NOT EXISTS FOR (ac:AssetClass) REQUIRE ac.name IS UNIQUE\",\n",
    "                \"CREATE CONSTRAINT IF NOT EXISTS FOR (inv:Investor) REQUIRE inv.investorId IS UNIQUE\", # New constraint for Investor\n",
    "                \"CREATE CONSTRAINT IF NOT EXISTS FOR (ent:__Entity__) REQUIRE ent.name IS UNIQUE\"\n",
    "            ]\n",
    "            for constraint in constraints:\n",
    "                session.run(constraint)\n",
    "            logging.info(\"✅ Investment KG constraints (updated) created/ensured.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"🛑 Error creating Investment KG constraints: {e}\")\n",
    "\n",
    "# Note: The clear_graph_data function was provided separately.\n",
    "# Ensure it's run before populate_investment_kg_updated if needed.\n",
    "\n",
    "def populate_investment_kg_updated(driver,\n",
    "                                   companies_details: List[Dict],\n",
    "                                   etf_details_df: pd.DataFrame,\n",
    "                                   investor_info_df: pd.DataFrame, # New parameter\n",
    "                                   investment_rels_df: pd.DataFrame # New parameter\n",
    "                                   ):\n",
    "    \"\"\"Populates the Neo4j graph with expanded investment data.\"\"\"\n",
    "    if not driver:\n",
    "        logging.error(\"Neo4j driver not available. Skipping KG population.\")\n",
    "        return\n",
    "\n",
    "    logging.info(\"\\n--- Populating Investment Knowledge Graph (Updated) ---\")\n",
    "    with driver.session(database=\"neo4j\") as session:\n",
    "        # 1. Process Companies and their details\n",
    "        logging.info(f\"Processing {len(companies_details)} companies...\")\n",
    "        for company_data in companies_details:\n",
    "            if company_data.get(\"error\"):\n",
    "                logging.warning(f\"  Skipping company {company_data.get('ticker', 'Unknown ticker')} due to previous fetch error.\")\n",
    "                continue\n",
    "\n",
    "            ticker = company_data[\"ticker\"]\n",
    "            company_name = company_data.get(\"name\", ticker)\n",
    "\n",
    "            company_props = {\n",
    "                \"ticker\": ticker,\n",
    "                \"name\": company_name,\n",
    "                \"sector\": company_data.get(\"sector\", \"N/A\"),\n",
    "                \"industry\": company_data.get(\"industry\", \"N/A\"),\n",
    "                \"country\": company_data.get(\"country\", \"N/A\"), # New property\n",
    "                \"summary\": company_data.get(\"summary\", \"N/A\"),\n",
    "                \"marketCap\": company_data.get(\"marketCap\"), # Already handled if None\n",
    "                \"trailingPE\": company_data.get(\"trailingPE\"),\n",
    "                \"forwardPE\": company_data.get(\"forwardPE\"),\n",
    "                \"beta\": company_data.get(\"beta\"),\n",
    "                \"website\": company_data.get(\"website\", \"N/A\"),\n",
    "                \"recommendationKey\": company_data.get(\"recommendationKey\", \"N/A\"),\n",
    "                \"earningsDate\": company_data.get(\"earningsDate\", \"N/A\"),\n",
    "                # investmentProfileSummary is added later by enrichment\n",
    "            }\n",
    "\n",
    "            # Remove None values from props to avoid issues with Neo4j properties\n",
    "            company_props_cleaned = {k: v for k, v in company_props.items() if v is not None}\n",
    "\n",
    "\n",
    "            company_cypher = \"\"\"\n",
    "            MERGE (c:Company {ticker: $props.ticker})\n",
    "            SET c = $props // Overwrites/sets all properties from the props map\n",
    "            WITH c\n",
    "            CALL apoc.create.addLabels(c, ['__Entity__']) YIELD node\n",
    "            SET node.name = $props.ticker\n",
    "            RETURN distinct 'company_done'\n",
    "            \"\"\"\n",
    "            session.run(company_cypher, props=company_props_cleaned)\n",
    "\n",
    "            # Sector node and relationship\n",
    "            if company_props_cleaned.get(\"sector\") and company_props_cleaned[\"sector\"] != \"N/A\":\n",
    "                session.run(\"\"\"\n",
    "                    MERGE (s:Sector {name: $sector_name}) ON CREATE SET s.name = $sector_name\n",
    "                    WITH s CALL apoc.create.addLabels(s, ['__Entity__']) YIELD node AS sector_node\n",
    "                    MATCH (c:Company {ticker: $company_ticker})\n",
    "                    MERGE (c)-[:OPERATES_IN_SECTOR]->(sector_node)\n",
    "                \"\"\", sector_name=company_props_cleaned[\"sector\"], company_ticker=ticker)\n",
    "\n",
    "            # Industry node and relationship\n",
    "            if company_props_cleaned.get(\"industry\") and company_props_cleaned[\"industry\"] != \"N/A\":\n",
    "                session.run(\"\"\"\n",
    "                    MERGE (i:Industry {name: $industry_name}) ON CREATE SET i.name = $industry_name\n",
    "                    WITH i CALL apoc.create.addLabels(i, ['__Entity__']) YIELD node AS industry_node\n",
    "                    MATCH (c:Company {ticker: $company_ticker})\n",
    "                    MERGE (c)-[:OPERATES_IN_INDUSTRY]->(industry_node)\n",
    "                \"\"\", industry_name=company_props_cleaned[\"industry\"], company_ticker=ticker)\n",
    "\n",
    "            # News Articles\n",
    "            for news_item in company_data.get(\"news\", []):\n",
    "                if news_item.get(\"link\") and news_item.get(\"title\"): # Ensure basic info exists\n",
    "                    publish_time_str = None\n",
    "                    if pd.notna(news_item.get(\"publishTime\")):\n",
    "                        if isinstance(news_item[\"publishTime\"], pd.Timestamp):\n",
    "                            publish_time_str = news_item[\"publishTime\"].isoformat()\n",
    "                        else: # Assume it's already an ISO string or compatible\n",
    "                            publish_time_str = str(news_item[\"publishTime\"])\n",
    "\n",
    "                    news_props = {\n",
    "                        \"link\": news_item[\"link\"],\n",
    "                        \"title\": news_item[\"title\"],\n",
    "                        \"name\": news_item[\"title\"], # For __Entity__\n",
    "                        \"publisher\": news_item.get(\"publisher\", \"N/A\"),\n",
    "                        \"publishTime\": publish_time_str\n",
    "                    }\n",
    "                    news_props_cleaned = {k: v for k, v in news_props.items() if v is not None}\n",
    "\n",
    "                    session.run(\"\"\"\n",
    "                        MERGE (n:NewsArticle {link: $props.link})\n",
    "                        SET n = $props\n",
    "                        WITH n CALL apoc.create.addLabels(n, ['__Entity__']) YIELD node AS news_node\n",
    "                        MATCH (c:Company {ticker: $company_ticker})\n",
    "                        MERGE (c)-[:MENTIONED_IN_NEWS]->(news_node)\n",
    "                    \"\"\", props=news_props_cleaned, company_ticker=ticker)\n",
    "            logging.info(f\"  Processed company: {company_name} ({ticker})\")\n",
    "\n",
    "        # 2. Process Investors\n",
    "        if not investor_info_df.empty:\n",
    "            logging.info(f\"\\nProcessing {len(investor_info_df)} investors...\")\n",
    "            for _, investor_row in investor_info_df.iterrows():\n",
    "                investor_props = {\n",
    "                    \"investorId\": investor_row[\"investorId\"],\n",
    "                    \"name\": investor_row[\"name\"], # Also for __Entity__\n",
    "                    \"type\": investor_row.get(\"type\", \"N/A\"),\n",
    "                    \"focus_areas\": investor_row.get(\"focus_areas\", \"N/A\")\n",
    "                }\n",
    "                investor_props_cleaned = {k: v for k, v in investor_props.items() if v is not None}\n",
    "                session.run(\"\"\"\n",
    "                    MERGE (inv:Investor {investorId: $props.investorId})\n",
    "                    SET inv = $props\n",
    "                    WITH inv CALL apoc.create.addLabels(inv, ['__Entity__']) YIELD node\n",
    "                    // node.name is already set via props.name\n",
    "                    RETURN distinct 'investor_done'\n",
    "                \"\"\", props=investor_props_cleaned)\n",
    "                logging.info(f\"  Processed investor: {investor_props_cleaned['name']}\")\n",
    "        else:\n",
    "            logging.warning(\"Investor DataFrame is empty. Skipping investor population.\")\n",
    "\n",
    "        # 3. Process Investment Relationships\n",
    "        if not investment_rels_df.empty:\n",
    "            logging.info(f\"\\nProcessing {len(investment_rels_df)} investment relationships...\")\n",
    "            for _, rel_row in investment_rels_df.iterrows():\n",
    "                session.run(\"\"\"\n",
    "                    MATCH (inv:Investor {investorId: $investorId})\n",
    "                    MATCH (c:Company {ticker: $companyTicker})\n",
    "                    MERGE (inv)-[r:INVESTED_IN]->(c)\n",
    "                    SET r.stage = $stage // Add stage as a relationship property\n",
    "                    RETURN distinct 'investment_rel_done'\n",
    "                \"\"\", investorId=rel_row[\"investorId\"], companyTicker=rel_row[\"companyTicker\"], stage=rel_row.get(\"stage\", \"N/A\"))\n",
    "            logging.info(\"  Investment relationships processed.\")\n",
    "        else:\n",
    "            logging.warning(\"Investment relationships DataFrame is empty. Skipping.\")\n",
    "\n",
    "        # 4. Process ETFs and their holdings\n",
    "        if not etf_details_df.empty:\n",
    "            logging.info(f\"\\nProcessing {len(etf_details_df)} ETFs...\")\n",
    "            for _, etf_row in etf_details_df.iterrows():\n",
    "                if pd.isna(etf_row.get(\"ticker\")) or pd.isna(etf_row.get(\"name\")):\n",
    "                    logging.warning(f\"  Skipping ETF due to missing ticker or name: {etf_row.get('ticker')}\")\n",
    "                    continue\n",
    "\n",
    "                etf_ticker = etf_row[\"ticker\"]\n",
    "                etf_name = etf_row[\"name\"]\n",
    "                etf_props = {\n",
    "                    \"ticker\": etf_ticker,\n",
    "                    \"name\": etf_name, # Also for __Entity__\n",
    "                    \"investmentFocus\": etf_row.get(\"investment_focus\", \"N/A\"),\n",
    "                    \"assetClass\": etf_row.get(\"asset_class\", \"N/A\"),\n",
    "                    \"family\": etf_row.get(\"family\", \"N/A\"), # New property\n",
    "                    \"expenseRatio\": etf_row.get(\"expenseRatio\"), # Already float or None\n",
    "                    \"AUM\": etf_row.get(\"AUM\") # Already float or None\n",
    "                }\n",
    "                etf_props_cleaned = {k: v for k, v in etf_props.items() if v is not None}\n",
    "\n",
    "                session.run(\"\"\"\n",
    "                    MERGE (e:ETF {ticker: $props.ticker})\n",
    "                    SET e = $props\n",
    "                    WITH e CALL apoc.create.addLabels(e, ['__Entity__']) YIELD node\n",
    "                    // node.name is already set via props.name\n",
    "                    RETURN distinct 'etf_done'\n",
    "                \"\"\", props=etf_props_cleaned)\n",
    "\n",
    "                # AssetClass node and relationship\n",
    "                if etf_props_cleaned.get(\"assetClass\") and etf_props_cleaned[\"assetClass\"] != \"N/A\":\n",
    "                     session.run(\"\"\"\n",
    "                        MERGE (ac:AssetClass {name: $ac_name}) ON CREATE SET ac.name = $ac_name\n",
    "                        WITH ac CALL apoc.create.addLabels(ac, ['__Entity__']) YIELD node AS ac_node\n",
    "                        MATCH (e:ETF {ticker: $etf_ticker})\n",
    "                        MERGE (e)-[:FOCUSES_ON_ASSET_CLASS]->(ac_node)\n",
    "                    \"\"\", ac_name=etf_props_cleaned[\"assetClass\"], etf_ticker=etf_ticker)\n",
    "\n",
    "                # Link ETF to Company (Stock) holdings using HOLDS_STOCK\n",
    "                for holding_ticker in etf_row.get(\"top_holdings_tickers\", []):\n",
    "                    session.run(\"\"\"\n",
    "                        MATCH (e:ETF {ticker: $etf_ticker})\n",
    "                        MATCH (c:Company {ticker: $company_ticker})\n",
    "                        MERGE (e)-[r:HOLDS_STOCK]->(c)\n",
    "                        SET r.source = 'sample_data'\n",
    "                    \"\"\", etf_ticker=etf_ticker, company_ticker=holding_ticker)\n",
    "                logging.info(f\"  Processed ETF: {etf_name} ({etf_ticker})\")\n",
    "        else:\n",
    "            logging.warning(\"ETF DataFrame is empty. Skipping ETF population.\")\n",
    "\n",
    "    logging.info(\"✅ Investment Knowledge Graph population complete (Updated).\")\n",
    "\n",
    "#--- Main Execution for KG Population (Example) ---\n",
    "#This would be in a separate cell after Cell 4 (Data Loading) and the cell for clearing the DB.\n",
    "if 'neo4j_driver' in locals() and neo4j_driver and \\\n",
    "   'detailed_companies_data' in locals() and \\\n",
    "   'etf_df' in locals() and \\\n",
    "   'investor_df' in locals() and \\\n",
    "   'investment_relationships_df' in locals():\n",
    "\n",
    "    # 1. Clear existing data (IMPORTANT before re-populating)\n",
    "    # clear_graph_data(neo4j_driver) # Call the function you have for this\n",
    "\n",
    "    # 2. Create constraints\n",
    "    create_investment_kg_constraints_updated(neo4j_driver)\n",
    "\n",
    "    # 3. Populate graph\n",
    "    populate_investment_kg_updated(\n",
    "        neo4j_driver,\n",
    "        detailed_companies_data,\n",
    "        etf_df,\n",
    "        investor_df,\n",
    "        investment_relationships_df\n",
    "    )\n",
    "else:\n",
    "    logging.warning(\"⚠️ Neo4j driver or necessary DataFrames not initialized. Cannot populate Investment KG.\")\n",
    "    logging.warning(f\"  Driver: {'Present' if 'neo4j_driver' in locals() and neo4j_driver else 'Missing'}\")\n",
    "    logging.warning(f\"  detailed_companies_data: {'Present' if 'detailed_companies_data' in locals() else 'Missing'}\")\n",
    "    logging.warning(f\"  etf_df: {'Present' if 'etf_df' in locals() else 'Missing'}\")\n",
    "    logging.warning(f\"  investor_df: {'Present' if 'investor_df' in locals() else 'Missing'}\")\n",
    "    logging.warning(f\"  investment_relationships_df: {'Present' if 'investment_relationships_df' in locals() else 'Missing'}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "evh4QF8R4c51",
   "metadata": {
    "id": "evh4QF8R4c51"
   },
   "source": [
    "### Verifying the Investment KG Structure\n",
    "\n",
    "Let's run some queries to ensure our Investment KG has been created as expected.\n",
    "\n",
    "*(This is a good point, Saumil, to remind your audience that visualizing parts of the graph in the Neo4j Browser can be very insightful for understanding these connections.)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cLS5RaGp4c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_cypher_query_for_verification(driver, query, params=None):\n",
    "    if not driver:\n",
    "        print(\"Neo4j driver not available.\")\n",
    "        return []\n",
    "    try:\n",
    "        with driver.session(database=\"neo4j\") as session:\n",
    "            result = session.run(query, params)\n",
    "            return [record.data() for record in result]\n",
    "    except Exception as e:\n",
    "        print(f\"🛑 Cypher query error: {e}\")\n",
    "        return []\n",
    "\n",
    "if neo4j_driver:\n",
    "    print(\"\\n--- Verifying Investment KG Structure ---\")\n",
    "\n",
    "    node_counts_query = \"\"\"\n",
    "    CALL db.labels() YIELD label\n",
    "    CALL apoc.cypher.run('MATCH (:`'+label+'`) RETURN count(*) AS count', {}) YIELD value\n",
    "    RETURN label, value.count AS count\n",
    "    ORDER BY label\n",
    "    \"\"\"\n",
    "    node_counts = run_cypher_query_for_verification(neo4j_driver, node_counts_query)\n",
    "    print(\"\\nNode Counts by Label:\")\n",
    "    if node_counts:\n",
    "        for item in node_counts:\n",
    "            print(f\"  - {item['label']}: {item['count']}\")\n",
    "    else:\n",
    "        print(\"  Could not retrieve node counts.\")\n",
    "\n",
    "    print(\"\\nSample ETFs and their Holdings (Company Tickers):\")\n",
    "    sample_etf_holdings = run_cypher_query_for_verification(neo4j_driver, \"\"\"\n",
    "        MATCH (e:ETF)-[:HOLDS_STOCK]->(c:Company)\n",
    "        RETURN e.name AS etf_name, e.ticker AS etf_ticker, collect(c.ticker) AS holdings_tickers\n",
    "        LIMIT 3\n",
    "    \"\"\")\n",
    "    if sample_etf_holdings:\n",
    "        for item in sample_etf_holdings:\n",
    "            print(f\"  - ETF: {item['etf_name']} ({item['etf_ticker']}), Holdings: {item['holdings_tickers']}\")\n",
    "    else:\n",
    "        print(\"  No ETF holdings data found or error in query.\")\n",
    "\n",
    "    ticker_for_news_check = detailed_companies_data[0]['ticker'] if detailed_companies_data else None\n",
    "    if ticker_for_news_check:\n",
    "        print(f\"\\nNews for Company {ticker_for_news_check}:\")\n",
    "        company_news = run_cypher_query_for_verification(neo4j_driver, \"\"\"\n",
    "            MATCH (c:Company {ticker: $ticker})-[:MENTIONED_IN_NEWS]->(n:NewsArticle)\n",
    "            RETURN n.title AS news_title, n.publisher AS publisher\n",
    "            LIMIT 3\n",
    "        \"\"\", params={\"ticker\": ticker_for_news_check})\n",
    "        if company_news:\n",
    "            for item in company_news:\n",
    "                print(f\"  - Title: {item['news_title']} (Publisher: {item['publisher']})\")\n",
    "        else:\n",
    "            print(f\"  No news found for {ticker_for_news_check} or error in query.\")\n",
    "    else:\n",
    "        print(\"\\nSkipping news check as no ticker was processed for detailed info.\")\n",
    "else:\n",
    "    print(\"⚠️ Neo4j driver not initialized. Cannot run Investment KG verification queries.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "avM6Jjt54c52",
   "metadata": {
    "id": "avM6Jjt54c52"
   },
   "source": [
    "## Part 3: LLM-Powered Graph Enrichment - Extracting Deeper Insights with AI\n",
    "\n",
    "Our Investment Knowledge Graph now contains foundational structured data. The next step, crucial for building truly intelligent systems, is to **enrich this graph using Large Language Models (LLMs)**. This involves leveraging LLMs to analyze existing data (both structured and unstructured) and infer new properties or relationships, making our KG more comprehensive and insightful.\n",
    "\n",
    "**Why is LLM-driven enrichment valuable in an investment context?**\n",
    "\n",
    "1.  **Sentiment Analysis at Scale:** LLMs can process news headlines or company reports to determine sentiment (positive, negative, neutral) towards a company or market trend. Storing this sentiment directly in the graph allows for powerful queries like, \"Show me companies in the tech sector with recent positive news sentiment and strong P/E ratios.\"\n",
    "2.  **Generating Actionable Summaries:** LLMs can synthesize information from multiple sources (e.g., a company's financial summary, recent news, sector trends) to create a concise `InvestmentProfileSummary` or `RiskOpportunityOutlook`. This provides analysts with quick, AI-generated perspectives.\n",
    "3.  **Inferring Latent Attributes:** Based on a company's description, industry, and news, an LLM might infer attributes not explicitly stated, such as \"PrimaryBusinessModel\" (e.g., SaaS, B2C Marketplace) or \"KeyTechnologicalFocus\" (e.g., AI-driven drug discovery, Quantum computing).\n",
    "\n",
    "**Our Enrichment Tasks for this Notebook:**\n",
    "\n",
    "For our Investment KG, we will use an LLM to:\n",
    "\n",
    "1.  **Infer Sentiment from News Headlines:** For each `NewsArticle` node linked to a `Company`, we'll use an LLM to classify the sentiment of the headline as `POSITIVE`, `NEGATIVE`, or `NEUTRAL`. This sentiment will be stored as a property on the `NewsArticle` node.\n",
    "2.  **Generate an `InvestmentProfileSummary` for Companies:** For each `Company` node, we'll prompt an LLM to create a brief summary. The LLM will consider the company's existing `summary` (from yfinance), its `sector`, `industry`, and the aggregated sentiment from its recent news.\n",
    "\n",
    "**The Engineering Perspective:**\n",
    "Implementing such enrichment pipelines requires careful prompt engineering, managing LLM API calls (cost and rate limits), and designing efficient ways to update the graph. For engineering leaders, this highlights the need for MLOps practices even when primarily using pre-trained LLMs for inference tasks. The ROI comes from the significantly enhanced query capabilities and the depth of insight the enriched KG can provide.\n",
    "\n",
    "Let's define our `InvestmentKGEnricher` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "DQh0g4ec4c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "class InvestmentKGEnricher:\n",
    "    def __init__(self, openai_client: OpenAI, neo4j_driver: GraphDatabase.driver):\n",
    "        self.client = openai_client\n",
    "        self.driver = neo4j_driver\n",
    "        self.model = \"gpt-3.5-turbo-0125\"\n",
    "\n",
    "    def _get_llm_response(self, prompt_messages: List[Dict], retries=2, delay=5):\n",
    "        if not self.client:\n",
    "            logging.warning(\"OpenAI client not initialized. Skipping LLM call.\")\n",
    "            return None\n",
    "        for attempt in range(retries):\n",
    "            try:\n",
    "                completion = self.client.chat.completions.create(\n",
    "                    model=self.model,\n",
    "                    messages=prompt_messages,\n",
    "                    temperature=0.3,\n",
    "                    response_format={\"type\": \"json_object\"}\n",
    "                )\n",
    "                content = completion.choices[0].message.content\n",
    "                return json.loads(content)\n",
    "            except json.JSONDecodeError as e_json:\n",
    "                logging.error(f\"LLM response was not valid JSON on attempt {attempt+1}: {content[:200]}... Error: {e_json}\")\n",
    "            except Exception as e:\n",
    "                logging.error(f\"LLM API call failed on attempt {attempt+1}: {e}\")\n",
    "            if attempt < retries - 1:\n",
    "                logging.info(f\"Retrying in {delay} seconds...\")\n",
    "                time.sleep(delay)\n",
    "            else:\n",
    "                logging.error(\"LLM call failed after multiple retries.\")\n",
    "        return None\n",
    "\n",
    "    def infer_news_sentiment(self, news_article_link: str, news_title: str) -> Optional[str]:\n",
    "        if not self.driver:\n",
    "            logging.warning(\"Neo4j driver not available. Skipping news sentiment inference.\")\n",
    "            return None\n",
    "\n",
    "        prompt_messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a financial news sentiment analyzer. Classify the sentiment of the given news headline as POSITIVE, NEGATIVE, or NEUTRAL. Return ONLY a JSON object with a single key 'sentiment' and one of these three values.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"News Headline: \\\"{news_title}\\\"\\n\\nJSON:\"}\n",
    "        ]\n",
    "\n",
    "        llm_response = self._get_llm_response(prompt_messages)\n",
    "\n",
    "        if llm_response and \"sentiment\" in llm_response:\n",
    "            sentiment = llm_response[\"sentiment\"].upper()\n",
    "            if sentiment not in [\"POSITIVE\", \"NEGATIVE\", \"NEUTRAL\"]:\n",
    "                logging.warning(f\"LLM returned unexpected sentiment '{sentiment}' for title '{news_title}'. Defaulting to NEUTRAL.\")\n",
    "                sentiment = \"NEUTRAL\"\n",
    "\n",
    "            try:\n",
    "                with self.driver.session(database=\"neo4j\") as session:\n",
    "                    session.run(\"\"\"\n",
    "                        MATCH (n:NewsArticle {link: $link})\n",
    "                        SET n.inferredSentiment = $sentiment\n",
    "                    \"\"\", link=news_article_link, sentiment=sentiment)\n",
    "                logging.info(f\"  Updated sentiment for news '{news_title[:50]}...' to {sentiment}.\")\n",
    "                return sentiment\n",
    "            except Exception as e:\n",
    "                logging.error(f\"  Failed to update sentiment in Neo4j for news '{news_title[:50]}...': {e}\")\n",
    "        else:\n",
    "            logging.warning(f\"  Could not determine sentiment for news '{news_title[:50]}...'. LLM response: {llm_response}\")\n",
    "        return None\n",
    "\n",
    "    def generate_company_investment_profile(self, company_ticker: str) -> Optional[str]:\n",
    "        if not self.driver:\n",
    "            logging.warning(\"Neo4j driver not available. Skipping company profile generation.\")\n",
    "            return None\n",
    "\n",
    "        query = \"\"\"\n",
    "        MATCH (c:Company {ticker: $ticker})\n",
    "        OPTIONAL MATCH (c)-[:MENTIONED_IN_NEWS]->(n:NewsArticle)\n",
    "        WHERE n.inferredSentiment IS NOT NULL\n",
    "        RETURN c.name AS name, c.summary AS yfinanceSummary, c.sector AS sector, c.industry AS industry,\n",
    "               collect(n.inferredSentiment) AS newsSentiments\n",
    "        LIMIT 1\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with self.driver.session(database=\"neo4j\") as session:\n",
    "                result = session.run(query, ticker=company_ticker).single()\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to fetch company data from Neo4j for {company_ticker}: {e}\")\n",
    "            return None\n",
    "\n",
    "        if not result:\n",
    "            logging.warning(f\"No data found in Neo4j for company ticker: {company_ticker}\")\n",
    "            return None\n",
    "\n",
    "        company_name = result.get(\"name\", company_ticker)\n",
    "        yfinance_summary = result.get(\"yfinanceSummary\", \"No summary available.\")\n",
    "        sector = result.get(\"sector\", \"N/A\")\n",
    "        industry = result.get(\"industry\", \"N/A\")\n",
    "        news_sentiments = result.get(\"newsSentiments\", [])\n",
    "\n",
    "        positive_count = news_sentiments.count(\"POSITIVE\")\n",
    "        negative_count = news_sentiments.count(\"NEGATIVE\")\n",
    "        overall_sentiment_hint = \"NEUTRAL\"\n",
    "        if positive_count > negative_count:\n",
    "            overall_sentiment_hint = \"LARGELY POSITIVE\"\n",
    "        elif negative_count > positive_count:\n",
    "            overall_sentiment_hint = \"LARGELY NEGATIVE\"\n",
    "        elif news_sentiments and positive_count == 0 and negative_count == 0:\n",
    "             overall_sentiment_hint = \"NEUTRAL\"\n",
    "        elif not news_sentiments:\n",
    "            overall_sentiment_hint = \"NOT AVAILABLE (NO RECENT NEWS WITH SENTIMENT)\"\n",
    "\n",
    "        prompt_messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a financial analyst providing concise investment profile summaries. Based on the provided company information and recent news sentiment, generate a 1-2 sentence 'investmentProfileSummary'. Focus on key characteristics, market position, or potential outlook. Output ONLY a JSON object with a single key 'investmentProfileSummary'.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"\"\"\n",
    "Company Name: {company_name}\n",
    "Sector: {sector}\n",
    "Industry: {industry}\n",
    "Existing Summary: {yfinance_summary[:1000]} (Truncated for brevity)\n",
    "Recent News Sentiment Trend: {overall_sentiment_hint}\n",
    "\n",
    "JSON:\n",
    "            \"\"\"}\n",
    "        ]\n",
    "\n",
    "        llm_response = self._get_llm_response(prompt_messages)\n",
    "\n",
    "        if llm_response and \"investmentProfileSummary\" in llm_response:\n",
    "            profile_summary = llm_response[\"investmentProfileSummary\"]\n",
    "            try:\n",
    "                with self.driver.session(database=\"neo4j\") as session:\n",
    "                    session.run(\"\"\"\n",
    "                        MATCH (c:Company {ticker: $ticker})\n",
    "                        SET c.investmentProfileSummary = $summary\n",
    "                    \"\"\", ticker=company_ticker, summary=profile_summary)\n",
    "                logging.info(f\"  Updated investment profile summary for {company_name}.\")\n",
    "                return profile_summary\n",
    "            except Exception as e:\n",
    "                logging.error(f\"  Failed to update profile summary in Neo4j for {company_name}: {e}\")\n",
    "        else:\n",
    "            logging.warning(f\"  Could not generate investment profile summary for {company_name}. LLM response: {llm_response}\")\n",
    "        return None\n",
    "\n",
    "    def enrich_investment_kg(self, company_tickers: List[str]):\n",
    "        if not self.driver or not self.client:\n",
    "            logging.error(\"Neo4j driver or OpenAI client not initialized. Aborting enrichment.\")\n",
    "            return\n",
    "\n",
    "        logging.info(\"--- Starting Investment KG Enrichment Process ---\")\n",
    "        for i, ticker in enumerate(company_tickers):\n",
    "            logging.info(f\"\\nProcessing enrichment for company ({i+1}/{len(company_tickers)}): {ticker}\")\n",
    "\n",
    "            news_to_process_query = \"\"\"\n",
    "            MATCH (c:Company {ticker: $ticker})-[:MENTIONED_IN_NEWS]->(n:NewsArticle)\n",
    "            WHERE n.inferredSentiment IS NULL AND n.title IS NOT NULL AND n.link IS NOT NULL\n",
    "            RETURN n.link AS link, n.title AS title\n",
    "            LIMIT 5\n",
    "            \"\"\"\n",
    "            try:\n",
    "                with self.driver.session(database=\"neo4j\") as session:\n",
    "                    news_results = session.run(news_to_process_query, ticker=ticker)\n",
    "                    news_items = [dict(record) for record in news_results]\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Failed to fetch news for {ticker} from Neo4j: {e}\")\n",
    "                continue\n",
    "\n",
    "            if news_items:\n",
    "                logging.info(f\"  Found {len(news_items)} news articles to analyze for sentiment for {ticker}.\")\n",
    "                for news_item in news_items:\n",
    "                    self.infer_news_sentiment(news_item[\"link\"], news_item[\"title\"])\n",
    "                    time.sleep(0.5)\n",
    "            else:\n",
    "                logging.info(f\"  No new news articles found to analyze for sentiment for {ticker}.\")\n",
    "\n",
    "            self.generate_company_investment_profile(ticker)\n",
    "\n",
    "            if (i + 1) % 2 == 0 and len(company_tickers) > 2:\n",
    "                logging.info(\"Pausing briefly to manage API rate limits...\")\n",
    "                time.sleep(3)\n",
    "\n",
    "        logging.info(\"--- Investment KG Enrichment Process Complete ---\")\n",
    "\n",
    "if neo4j_driver and openai_client:\n",
    "    enricher = InvestmentKGEnricher(openai_client=openai_client, neo4j_driver=neo4j_driver)\n",
    "\n",
    "    if 'detailed_companies_data' in locals() and detailed_companies_data:\n",
    "        tickers_to_enrich = [comp['ticker'] for comp in detailed_companies_data if not comp.get(\"error\")]\n",
    "        if tickers_to_enrich:\n",
    "            enricher.enrich_investment_kg(tickers_to_enrich)\n",
    "        else:\n",
    "            logging.warning(\"No valid company tickers found from yfinance fetching to enrich.\")\n",
    "    else:\n",
    "        logging.warning(\"Variable 'detailed_companies_data' not found or empty. Skipping enrichment.\")\n",
    "\n",
    "else:\n",
    "    logging.error(\"Neo4j driver or OpenAI client not initialized. Cannot run enrichment.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "LQdd7L0h4c55",
   "metadata": {
    "id": "LQdd7L0h4c55"
   },
   "source": [
    "### Verifying Enriched Investment KG\n",
    "\n",
    "Let's check our graph again to see the newly added `inferredSentiment` on `NewsArticle` nodes and the `investmentProfileSummary` on `Company` nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "DTfvP80u4c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_cypher_query_for_enrichment_verification(driver, query, params=None):\n",
    "    if not driver:\n",
    "        print(\"Neo4j driver not available.\")\n",
    "        return []\n",
    "    try:\n",
    "        with driver.session(database=\"neo4j\") as session:\n",
    "            result = session.run(query, params)\n",
    "            return [record.data() for record in result]\n",
    "    except Exception as e:\n",
    "        print(f\"🛑 Cypher query error: {e}\")\n",
    "        return []\n",
    "\n",
    "if neo4j_driver:\n",
    "    print(\"\\n--- Verifying Enriched Investment KG Data ---\")\n",
    "\n",
    "    if 'detailed_companies_data' in locals() and detailed_companies_data:\n",
    "        ticker_to_check = detailed_companies_data[0]['ticker']\n",
    "        print(f\"\\nNews Sentiment for Company {ticker_to_check}:\")\n",
    "        news_sentiments = run_cypher_query_for_enrichment_verification(neo4j_driver, \"\"\"\n",
    "            MATCH (c:Company {ticker: $ticker})-[:MENTIONED_IN_NEWS]->(n:NewsArticle)\n",
    "            WHERE n.inferredSentiment IS NOT NULL\n",
    "            RETURN n.title AS news_title, n.inferredSentiment AS sentiment\n",
    "            LIMIT 5\n",
    "        \"\"\", params={\"ticker\": ticker_to_check})\n",
    "        if news_sentiments:\n",
    "            for item in news_sentiments:\n",
    "                print(f\"  - Title: {item['news_title'][:70]}... Sentiment: {item['sentiment']}\")\n",
    "        else:\n",
    "            print(f\"  No news with inferred sentiment found for {ticker_to_check}.\")\n",
    "\n",
    "    print(\"\\nCompany Investment Profile Summaries (first 3 enriched):\")\n",
    "    enriched_company_profiles = run_cypher_query_for_enrichment_verification(neo4j_driver, \"\"\"\n",
    "        MATCH (c:Company)\n",
    "        WHERE c.investmentProfileSummary IS NOT NULL\n",
    "        RETURN c.name AS company, c.investmentProfileSummary AS profile_summary\n",
    "        LIMIT 3\n",
    "    \"\"\")\n",
    "    if enriched_company_profiles:\n",
    "        for item in enriched_company_profiles:\n",
    "            print(f\"  - Company: {item['company']}\")\n",
    "            print(f\"    Profile Summary: {item['profile_summary']}\")\n",
    "    else:\n",
    "        print(\"  No company investment profile summaries found.\")\n",
    "else:\n",
    "    print(\"⚠️ Neo4j driver not initialized. Cannot run verification queries for enriched data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "-ViIxjDi4c57",
   "metadata": {
    "id": "-ViIxjDi4c57"
   },
   "source": [
    "## Part 4: Vector Indexing - Enabling Semantic Search on the KG\n",
    "\n",
    "Our Knowledge Graph (KG) now holds a wealth of structured and semi-structured information, including company details, financial metrics, news headlines, inferred sentiment, and LLM-generated summaries. To unlock its full potential, we need to be able to query it not just based on exact matches or explicit relationships, but also based on *semantic similarity* or conceptual meaning.\n",
    "\n",
    "This is where **vector indexing** comes into play.\n",
    "\n",
    "**What are Vector Embeddings and Vector Indexes?**\n",
    "\n",
    "1.  **Vector Embeddings:** These are numerical representations (lists of numbers, or vectors) of text or other data. LLMs (or specialized embedding models) are used to convert textual information (like company summaries, news titles, or even user queries) into these dense vector embeddings. The key idea is that texts with similar meanings will have mathematically similar vectors.\n",
    "2.  **Vector Index:** A specialized data structure built within Neo4j (or other databases) that stores these embeddings and allows for very fast similarity searches. Given a query vector, the index can quickly find the N most similar vectors (and thus the N most semantically similar nodes) in the graph.\n",
    "\n",
    "**Why Use Vector Indexing on Our Investment KG?**\n",
    "\n",
    "* **Semantic Search for Companies/ETFs:** Find companies or ETFs based on conceptual descriptions, even if the exact keywords aren't present. For example: \"Find innovative tech companies focused on sustainable energy solutions\" or \"Show me ETFs that align with a conservative growth strategy.\"\n",
    "* **Linking Unstructured Queries to Structured Data:** A user might ask a vague question. Vector search can identify the most relevant nodes (e.g., specific companies or industries) in the graph, which can then be used as starting points for more precise, structured Cypher queries.\n",
    "* **Content-Based Recommendation:** Find companies similar to a given company based on their summaries, news sentiment, or strategic focus areas.\n",
    "\n",
    "**Implementation in Neo4j:**\n",
    "Neo4j allows you to create vector indexes on node properties. We'll create an index on our `__Entity__` nodes, leveraging properties like `name`, `summary`, `investmentProfileSummary`, `title`, and `investmentFocus`.\n",
    "\n",
    "**The Engineering Value:**\n",
    "Integrating vector search directly within the graph database offers significant advantages over maintaining separate vector and graph databases. It simplifies the architecture, reduces data synchronization issues, and allows for powerful hybrid queries that combine graph patterns with semantic similarity in a single operation. This leads to more efficient, scalable, and maintainable AI systems.\n",
    "\n",
    "Let's create the vector index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6yjR4BRr4c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Neo4jVector\n",
    "\n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    logging.warning(\"OPENAI_API_KEY environment variable not set. Langchain components might fail.\")\n",
    "    if 'openai_client' in locals() and openai_client and openai_client.api_key:\n",
    "         os.environ[\"OPENAI_API_KEY\"] = openai_client.api_key\n",
    "         logging.info(\"Set OPENAI_API_KEY for Langchain from initialized openai_client.\")\n",
    "\n",
    "def create_neo4j_vector_index(driver, openai_api_key_for_lc):\n",
    "    if not driver:\n",
    "        logging.error(\"Neo4j driver not available. Cannot create vector index.\")\n",
    "        return None\n",
    "    if not openai_api_key_for_lc:\n",
    "        logging.error(\"OpenAI API Key not available. Cannot create embeddings for vector index.\")\n",
    "        return None\n",
    "\n",
    "    embedding_node_property = 'embedding'\n",
    "    index_name = 'investmentEntityIndex'\n",
    "\n",
    "    logging.info(\"Updating nodes with a 'searchableDescription' property for consistent embedding...\")\n",
    "    try:\n",
    "        with driver.session(database=\"neo4j\") as session:\n",
    "            session.run(\"\"\"\n",
    "                MATCH (c:Company) WHERE c.summary IS NOT NULL OR c.investmentProfileSummary IS NOT NULL\n",
    "                SET c.searchableDescription = coalesce(c.name, \"\") + \" | Industry: \" + coalesce(c.industry, \"\") + \". Summary: \" + coalesce(c.investmentProfileSummary, coalesce(c.summary, \"\"))\n",
    "            \"\"\")\n",
    "            session.run(\"\"\"\n",
    "                MATCH (e:ETF) WHERE e.investmentFocus IS NOT NULL\n",
    "                SET e.searchableDescription = coalesce(e.name, \"\") + \" | Focus: \" + coalesce(e.investmentFocus, \"\")\n",
    "            \"\"\")\n",
    "            session.run(\"\"\"\n",
    "                MATCH (n:NewsArticle) WHERE n.title IS NOT NULL\n",
    "                SET n.searchableDescription = coalesce(n.title, \"\")\n",
    "            \"\"\")\n",
    "            session.run(\"MATCH (s:Sector) WHERE s.name IS NOT NULL SET s.searchableDescription = s.name\")\n",
    "            session.run(\"MATCH (i:Industry) WHERE i.name IS NOT NULL SET i.searchableDescription = i.name\")\n",
    "        logging.info(\"✅ 'searchableDescription' property updated/created for relevant nodes.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"🛑 Error creating 'searchableDescription': {e}\")\n",
    "        logging.warning(\"Proceeding with vector index creation, but embeddings might be suboptimal.\")\n",
    "\n",
    "    logging.info(f\"Attempting to create/retrieve Neo4jVector index: '{index_name}'\")\n",
    "    try:\n",
    "        if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "             os.environ[\"OPENAI_API_KEY\"] = openai_api_key_for_lc\n",
    "\n",
    "        embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "        vector_store = Neo4jVector.from_existing_graph(\n",
    "            embedding=embeddings,\n",
    "            url=NEO4J_URI,\n",
    "            username=NEO4J_USERNAME,\n",
    "            password=NEO4J_PASSWORD,\n",
    "            index_name=index_name,\n",
    "            node_label=\"__Entity__\",\n",
    "            text_node_properties=[\"searchableDescription\"],\n",
    "            embedding_node_property=embedding_node_property,\n",
    "            database=\"neo4j\"\n",
    "        )\n",
    "        logging.info(f\"✅ Vector index '{index_name}' created/retrieved successfully.\")\n",
    "        return vector_store\n",
    "    except Exception as e:\n",
    "        logging.error(f\"🛑 Failed to create/retrieve Neo4jVector index: {e}\")\n",
    "        return None\n",
    "\n",
    "vector_retriever_store = None\n",
    "if neo4j_driver and OPENAI_API_KEY:\n",
    "    logging.info(\"Creating/retrieving vector index. This may take some time if populating for the first time...\")\n",
    "    vector_retriever_store = create_neo4j_vector_index(neo4j_driver, OPENAI_API_KEY)\n",
    "else:\n",
    "    logging.warning(\"Neo4j driver or OpenAI API Key not available. Skipping vector index creation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6BoL2bql4c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "def test_similarity_search(vector_store_instance, query: str, k: int = 3):\n",
    "    if not vector_store_instance:\n",
    "        logging.error(\"Vector store instance not available. Cannot perform similarity search.\")\n",
    "        return []\n",
    "\n",
    "    logging.info(f\"\\nPerforming similarity search for query: '{query}' (top {k} results)\")\n",
    "    try:\n",
    "        results = vector_store_instance.similarity_search_with_score(query, k=k)\n",
    "        if results:\n",
    "            logging.info(\"✅ Similarity search successful. Results:\")\n",
    "            for i, (doc, score) in enumerate(results):\n",
    "                logging.info(f\"  Result {i+1} (Score: {score:.4f}):\")\n",
    "                logging.info(f\"    Node ID: {doc.metadata.get('element_id', 'N/A')}\")\n",
    "                logging.info(f\"    Labels: {doc.metadata.get('_labels', 'N/A')}\")\n",
    "                logging.info(f\"    Content (searchableDescription): {doc.page_content[:200]}...\")\n",
    "                if 'name' in doc.metadata:\n",
    "                     logging.info(f\"    Name: {doc.metadata['name']}\")\n",
    "                if 'ticker' in doc.metadata:\n",
    "                     logging.info(f\"    Ticker: {doc.metadata['ticker']}\")\n",
    "        else:\n",
    "            logging.warning(\"Similarity search returned no results.\")\n",
    "        return results\n",
    "    except Exception as e:\n",
    "        logging.error(f\"🛑 Error during similarity search: {e}\")\n",
    "        return []\n",
    "\n",
    "if 'vector_retriever_store' in locals() and vector_retriever_store:\n",
    "    test_query_1 = \"Technology companies innovating in artificial intelligence\"\n",
    "    search_results_1 = test_similarity_search(vector_retriever_store, test_query_1)\n",
    "\n",
    "    test_query_2 = \"ETFs focusing on sustainable and ESG investments\"\n",
    "    search_results_2 = test_similarity_search(vector_retriever_store, test_query_2)\n",
    "\n",
    "    if 'detailed_companies_data' in locals() and detailed_companies_data and detailed_companies_data[0].get(\"news\"):\n",
    "        first_company_news_title_snippet = detailed_companies_data[0][\"news\"][0][\"title\"][:30] if detailed_companies_data[0][\"news\"] else \"market performance\"\n",
    "        test_query_3 = f\"Recent news about {first_company_news_title_snippet}\"\n",
    "        search_results_3 = test_similarity_search(vector_retriever_store, test_query_3)\n",
    "    else:\n",
    "        logging.info(\"Skipping news-related vector search test as no news data was processed.\")\n",
    "else:\n",
    "    logging.warning(\"Vector store not initialized. Skipping similarity search tests.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "OObwAU3t4c59",
   "metadata": {
    "id": "OObwAU3t4c59"
   },
   "source": [
    "## Part 5: Advanced Retrieval Strategies - Querying the Investment KG with Intelligence\n",
    "\n",
    "Our Investment Knowledge Graph (KG) is now populated with structured data, LLM-enriched insights (like company summaries and news sentiment), and a vector index for semantic search. The next crucial step is to implement sophisticated retrieval strategies that can effectively harness this rich, interconnected data to answer complex business questions.\n",
    "\n",
    "We'll focus on two powerful approaches:\n",
    "\n",
    "1.  **Text2Cypher Retriever:** This technique uses a Large Language Model (LLM) to translate a user's natural language question into a precise Cypher query (Neo4j's graph query language). This allows users to interact with the graph without needing to know Cypher, enabling highly specific and complex data retrieval based on graph patterns and relationships.\n",
    "    * **Business Value:** Empowers business analysts or decision-makers to ask ad-hoc, detailed questions about company relationships, investment patterns, market structures, etc., that would be difficult or impossible with traditional SQL or simple keyword searches.\n",
    "\n",
    "2.  **Vector Similarity Retriever (Graph-Aware):** This retriever uses the vector index to find nodes (Companies, ETFs, News) whose textual descriptions are semantically similar to the user's query.\n",
    "    * **Business Value:** Useful for exploratory queries, finding entities based on conceptual descriptions rather than exact keywords, or for content-based recommendations.\n",
    "\n",
    "**The Engineering Perspective:**\n",
    "Implementing these retrievers involves careful prompt engineering for Text2Cypher, efficient interaction with the Neo4j database, and structuring the retrieved information so it can be effectively used by an LLM to generate a final answer. Performance considerations include optimizing Cypher queries generated by the LLM and managing the cost of LLM calls.\n",
    "\n",
    "Let's start by building the `Text2CypherRetriever`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "JwlJSivmnxr0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import json\n",
    "from neo4j import GraphDatabase\n",
    "from openai import OpenAI\n",
    "from typing import List, Dict, Any, Optional\n",
    "import time\n",
    "\n",
    "class Text2CypherInvestmentRetriever:\n",
    "    \"\"\"\n",
    "    Converts natural language questions about the Investment KG into Cypher queries using LLMs,\n",
    "    executes them, and can synthesize answers.\n",
    "\n",
    "    Mermaid diagram for flow:\n",
    "    ```mermaid\n",
    "    graph TD\n",
    "        A[User Question (Natural Language)] --> B{Text2CypherInvestmentRetriever Instance};\n",
    "        B -- 1. Calls `generate_cypher_query` --> C[generate_cypher_query Method];\n",
    "        C -- a. Accesses/Formats --> D[Graph Schema (from Neo4j)];\n",
    "        C -- b. Constructs Prompt with Question & Schema --> E[LLM (Cypher Gen Model)];\n",
    "        E -- Returns --> F[Generated Cypher Query String];\n",
    "        F --> B;\n",
    "        B -- 2. Calls `execute_cypher` with Cypher --> G[execute_cypher Method];\n",
    "        G -- Executes Query --> H[(Neo4j Database)];\n",
    "        H -- Returns Raw Results --> I[Query Results (Structured Data)];\n",
    "        I --> B;\n",
    "        B -- 3. Calls `synthesize_answer` with Question & Results --> J[synthesize_answer Method];\n",
    "        J -- Constructs Prompt with Question & Results --> K[LLM (Answer Gen Model)];\n",
    "        K -- Returns --> L[Synthesized Natural Language Answer];\n",
    "        L --> M[Output to User];\n",
    "    ```\n",
    "    \"\"\"\n",
    "    def __init__(self, neo4j_driver: GraphDatabase.driver, openai_client: OpenAI, graph_schema: Optional[str] = None):\n",
    "        self.driver = neo4j_driver\n",
    "        self.client = openai_client\n",
    "        self._schema = graph_schema\n",
    "        self.cypher_gen_model = \"gpt-4o-mini\"\n",
    "        self.answer_gen_model = \"gpt-4.1-nano\"\n",
    "\n",
    "    @property\n",
    "    def schema(self) -> str:\n",
    "        \"\"\"Lazily fetches and formats the Neo4j schema if not provided.\"\"\"\n",
    "        if self._schema is None:\n",
    "            if not self.driver:\n",
    "                logging.error(\"Neo4j driver not available to fetch schema.\")\n",
    "                return \"Schema not available.\"\n",
    "            try:\n",
    "                logging.info(\"Fetching graph schema from Neo4j...\")\n",
    "                with self.driver.session(database=\"neo4j\") as session:\n",
    "                    schema_query = \"CALL db.schema.visualization() YIELD nodes, relationships RETURN nodes, relationships\"\n",
    "                    result = session.run(schema_query).single()\n",
    "                    if not result: return \"Could not retrieve schema.\"\n",
    "\n",
    "                    nodes_schema = []\n",
    "                    valid_nodes = [node for node in result.get(\"nodes\", []) if node is not None and node.get('name') is not None]\n",
    "                    sorted_nodes = sorted(valid_nodes, key=lambda x: x.get('name', \"\"))\n",
    "                    for node in sorted_nodes:\n",
    "                        label = node.get('name', 'UnknownLabel')\n",
    "                        properties = sorted([\n",
    "                            prop['property'] for prop in node.get('properties', [])\n",
    "                            if prop['property'] not in ['embedding', 'searchableDescription']\n",
    "                        ])\n",
    "                        prop_str = f\", Properties: {properties}\" if properties else \"\"\n",
    "                        id_prop_note = \"\"\n",
    "                        if not properties:\n",
    "                           if label == \"Company\" or label == \"ETF\": id_prop_note = f\" (Primarily identified by 'ticker')\"\n",
    "                           elif label == \"NewsArticle\": id_prop_note = f\" (Primarily identified by 'link')\"\n",
    "                           elif label == \"Investor\": id_prop_note = f\" (Primarily identified by 'investorId')\"\n",
    "                           else: id_prop_note = f\" (Primarily identified by 'name')\"\n",
    "                        nodes_schema.append(f\"Node Label: `{label}`{prop_str}{id_prop_note}\")\n",
    "\n",
    "                    rels_schema = []\n",
    "                    valid_relationships = [rel for rel in result.get(\"relationships\", []) if rel is not None and rel.get('type') is not None]\n",
    "                    sorted_relationships = sorted(valid_relationships, key=lambda x: x.get('type', \"\"))\n",
    "                    for rel in sorted_relationships:\n",
    "                        rel_type = rel.get('type', 'UnknownRelationshipType')\n",
    "                        start_node_label = rel.get('startNodeLabel', 'ANY')\n",
    "                        end_node_label = rel.get('endNodeLabel', 'ANY')\n",
    "                        rel_props = sorted([p_info['property'] for p_info in rel.get('properties', []) if not p_info['property'].startswith('_')])\n",
    "                        prop_string = f\", Properties: {rel_props}\" if rel_props else \"\"\n",
    "                        rels_schema.append(f\"Relationship Type: `:{rel_type}` (Connects `{start_node_label}` to `{end_node_label}`{prop_string})\")\n",
    "\n",
    "                    self._schema = \"Graph Schema Overview:\\nNodes:\\n\" + \"\\n\".join(nodes_schema) + \"\\n\\nRelationships:\\n\" + \"\\n\".join(list(set(rels_schema)))\n",
    "                    logging.info(\"✅ Graph schema fetched and formatted for Text2Cypher.\")\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Failed to fetch schema from Neo4j: {e}\")\n",
    "                self._schema = \"Error fetching schema. Queries may be suboptimal.\"\n",
    "        return self._schema\n",
    "\n",
    "    def _get_llm_response_for_cypher(self, prompt_messages: List[Dict], retries=2, delay=5) -> Optional[str]:\n",
    "        if not self.client:\n",
    "            logging.warning(\"OpenAI client not initialized. Skipping Cypher generation.\")\n",
    "            return None\n",
    "        for attempt in range(retries):\n",
    "            try:\n",
    "                completion = self.client.chat.completions.create(\n",
    "                    model=self.cypher_gen_model,\n",
    "                    messages=prompt_messages,\n",
    "                    temperature=0.0,\n",
    "                )\n",
    "                cypher_query = completion.choices[0].message.content.strip()\n",
    "                if cypher_query.startswith(\"```cypher\"):\n",
    "                    cypher_query = cypher_query[len(\"```cypher\"):].strip()\n",
    "                if cypher_query.startswith(\"```\"):\n",
    "                    cypher_query = cypher_query[3:].strip()\n",
    "                if cypher_query.endswith(\"```\"):\n",
    "                    cypher_query = cypher_query[:-3].strip()\n",
    "                return cypher_query\n",
    "            except Exception as e:\n",
    "                logging.error(f\"LLM Cypher generation call failed on attempt {attempt+1}: {e}\")\n",
    "            if attempt < retries - 1:\n",
    "                logging.info(f\"Retrying Cypher generation in {delay} seconds...\")\n",
    "                time.sleep(delay)\n",
    "        logging.error(\"LLM Cypher generation failed after multiple retries.\")\n",
    "        return None\n",
    "\n",
    "    def generate_cypher_query(self, question: str) -> Optional[str]:\n",
    "        system_prompt = f\"\"\"You are an expert Neo4j Cypher query writer specializing in financial and investment data.\n",
    "Your task is to translate natural language questions into executable Cypher queries for a graph with the following schema details:\n",
    "{self.schema}\n",
    "\n",
    "Important Guidelines & Entity Identification:\n",
    "- Company nodes (`Company`) are primarily identified by their 'ticker' property (e.g., {{ticker: 'AAPL'}}). Other properties include 'name', 'sector', 'industry', 'country', 'summary', 'investmentProfileSummary', 'marketCap', 'trailingPE', 'forwardPE', 'beta', 'website', 'recommendationKey', 'earningsDate'.\n",
    "- ETF nodes (`ETF`) are identified by their 'ticker' property (e.g., {{ticker: 'SPY'}}). Other properties include 'name', 'investmentFocus', 'assetClass', 'family', 'expenseRatio', 'AUM', 'description_from_funds_data'.\n",
    "- Investor nodes (`Investor`) are identified by their 'investorId' property. Other properties include 'name', 'type', 'focus_areas'.\n",
    "- NewsArticle nodes (`NewsArticle`) are identified by their 'link' property. Other properties include 'title', 'publisher', 'publishTime', 'inferredSentiment' (POSITIVE, NEGATIVE, NEUTRAL).\n",
    "- Sector nodes (`Sector`), Industry nodes (`Industry`), and AssetClass nodes (`AssetClass`) are identified by their 'name' property.\n",
    "\n",
    "Relationships to use:\n",
    "- `OPERATES_IN_SECTOR` (Company -> Sector)\n",
    "- `OPERATES_IN_INDUSTRY` (Company -> Industry)\n",
    "- `MENTIONED_IN_NEWS` (Company -> NewsArticle)\n",
    "- `HOLDS_STOCK` (ETF -> Company)\n",
    "- `INVESTED_IN` (Investor -> Company), this relationship has a 'stage' property (e.g., {{stage: 'Series A'}}).\n",
    "- `FOCUSES_ON_ASSET_CLASS` (ETF -> AssetClass)\n",
    "\n",
    "Query Construction Rules:\n",
    "- Avoid using 'embedding' or 'searchableDescription' properties in MATCH clauses or WHERE conditions.\n",
    "- When returning node properties, use explicit aliases without dots (e.g., `RETURN i.name AS investorName, i.type AS investorType`). This helps the answer synthesis step.\n",
    "- For string comparisons (names, sectors, etc.), ALWAYS use case-insensitive matching: `toLower(node.property) CONTAINS toLower('search_term')` or `toLower(node.property) = toLower('exact_term')`.\n",
    "- For \"latest news\", use `ORDER BY n.publishTime DESC`. Return a few (e.g., 3) items if \"latest\" is not strictly singular.\n",
    "- Prioritize `ticker` for `Company`/`ETF` and `investorId` for `Investor` in MATCH patterns.\n",
    "- Output ONLY the Cypher query.\n",
    "\"\"\"\n",
    "\n",
    "        prompt_messages = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": f\"Natural language question: \\\"{question}\\\"\\n\\nCypher Query:\"}\n",
    "        ]\n",
    "\n",
    "        cypher_query = self._get_llm_response_for_cypher(prompt_messages)\n",
    "        if cypher_query:\n",
    "            logging.info(f\"Generated Cypher for '{question}':\\n{cypher_query}\")\n",
    "        return cypher_query\n",
    "\n",
    "    def execute_cypher(self, cypher_query: str) -> List[Dict[str, Any]]:\n",
    "        if not self.driver or not cypher_query:\n",
    "            logging.error(\"Neo4j driver or Cypher query not available.\")\n",
    "            return []\n",
    "        try:\n",
    "            with self.driver.session(database=\"neo4j\") as session:\n",
    "                result = session.run(cypher_query)\n",
    "                records = [record.data() for record in result]\n",
    "                logging.info(f\"Cypher query executed successfully. Fetched {len(records)} records.\")\n",
    "                return records\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error executing Cypher query \\\"{cypher_query[:100]}...\\\": {e}\")\n",
    "            return [{\"error\": str(e)}]\n",
    "\n",
    "    def synthesize_answer(self, question: str, query_results: List[Dict[str, Any]]) -> str:\n",
    "        if not self.client:\n",
    "            logging.warning(\"OpenAI client not initialized. Skipping answer synthesis.\")\n",
    "            return \"Could not synthesize answer: OpenAI client not available.\"\n",
    "\n",
    "        logging.info(f\"Data for synthesis for question '{question}': {json.dumps(query_results, indent=2, default=str)}\")\n",
    "\n",
    "        if query_results and isinstance(query_results[0], dict) and \"error\" in query_results[0]:\n",
    "            error_message = query_results[0][\"error\"]\n",
    "            logging.warning(f\"Error in query execution results for '{question}'. Error: {error_message}\")\n",
    "            return f\"I couldn't find specific information for your query due to an error during data retrieval: {error_message}. Please try rephrasing or check the system logs.\"\n",
    "\n",
    "        if not query_results:\n",
    "             logging.warning(f\"No results found to synthesize answer for '{question}'. Query returned an empty set.\")\n",
    "             return \"Based on the information in the knowledge graph, I could not find any specific data matching your query.\"\n",
    "\n",
    "        results_for_prompt = query_results[:10]\n",
    "        results_str = json.dumps(results_for_prompt, indent=2, default=str)\n",
    "\n",
    "        synthesis_system_prompt = \"\"\"You are a financial analyst assistant. Your task is to answer the user's question based *only* on the structured data provided.\n",
    "\n",
    "Instructions:\n",
    "1.  Examine the \"Original Question\" and the \"Retrieved Data\" (in JSON format).\n",
    "2.  If the \"Retrieved Data\" is an empty list `[]`, respond with: \"Based on the information in the knowledge graph, I could not find any specific data matching your query.\"\n",
    "3.  If the \"Retrieved Data\" is not empty:\n",
    "    a.  If the question asks \"Which [items]...\" (e.g., \"Which investors...\", \"Which companies...\"), and the data is a list of objects, list the relevant information from these objects. For example, if data is `[{\"investorName\": \"Alpha Fund\", \"investorType\": \"VC\"}, {\"investorName\": \"Beta Capital\", \"investorType\": \"PE\"}]` and the question was \"Which investors...\", your answer should be something like: \"The investors who funded both are Alpha Fund (Type: VC) and Beta Capital (Type: PE).\" or \"The following investors were found: Alpha Fund (VC), Beta Capital (PE).\" Ensure you clearly state what the listed items represent in relation to the question.\n",
    "    b.  If the question asks for a count (e.g., \"How many companies...\") and the data provides a count (e.g., `[{\"numberOfCompanies\": 3}]`), state the count: \"There are 3 such companies.\"\n",
    "    c.  For other types of questions, synthesize a concise answer using the properties present in the data.\n",
    "4.  If the data is present but doesn't seem to directly or fully answer the question, state what you found and mention that it might not be a complete answer to the specific question.\n",
    "5.  Do NOT make up information or infer beyond what is explicitly in the \"Retrieved Data\".\n",
    "6.  Be professional and direct. Use the property names in the JSON (e.g., `investorName`, `investorType`, `sectorName`) to understand the data.\n",
    "\"\"\"\n",
    "        prompt_messages = [\n",
    "            {\"role\": \"system\", \"content\": synthesis_system_prompt},\n",
    "            {\"role\": \"user\", \"content\": f\"\"\"\n",
    "Original Question: {question}\n",
    "\n",
    "Retrieved Data (JSON format):\n",
    "{results_str}\n",
    "\n",
    "Answer:\n",
    "\"\"\"}\n",
    "        ]\n",
    "\n",
    "        try:\n",
    "            completion = self.client.chat.completions.create(\n",
    "                model=self.answer_gen_model,\n",
    "                messages=prompt_messages,\n",
    "                temperature=0.05\n",
    "            )\n",
    "            answer = completion.choices[0].message.content.strip()\n",
    "            logging.info(f\"Synthesized answer for '{question}': {answer}\")\n",
    "            return answer\n",
    "        except Exception as e:\n",
    "            logging.error(f\"LLM answer synthesis call failed: {e}\")\n",
    "            return \"There was an issue generating the final answer.\"\n",
    "\n",
    "    def query_kg(self, question: str) -> Dict[str, Any]:\n",
    "        logging.info(f\"\\n--- Text2Cypher Processing Question: \\\"{question}\\\" ---\")\n",
    "        current_schema = self.schema\n",
    "        if \"Schema not available\" in current_schema or \"Error fetching schema\" in current_schema:\n",
    "            logging.error(\"Cannot generate Cypher query due to schema issues.\")\n",
    "            return {\n",
    "                \"question\": question, \"cypher_query\": None, \"results_count\":0, \"raw_results\": [],\n",
    "                \"answer\": \"System error: Could not retrieve graph schema to process your query.\", \"retrieval_method\": \"Text2Cypher\"\n",
    "            }\n",
    "\n",
    "        generated_cypher = self.generate_cypher_query(question)\n",
    "\n",
    "        if not generated_cypher:\n",
    "            return {\n",
    "                \"question\": question, \"cypher_query\": None, \"results_count\": 0,\n",
    "                \"raw_results\": [], \"answer\": \"Failed to generate a Cypher query for your question.\",\n",
    "                \"retrieval_method\": \"Text2Cypher\"\n",
    "            }\n",
    "\n",
    "        results = self.execute_cypher(generated_cypher)\n",
    "        answer = self.synthesize_answer(question, results)\n",
    "\n",
    "        return {\n",
    "            \"question\": question, \"cypher_query\": generated_cypher,\n",
    "            \"results_count\": len(results) if not (results and isinstance(results[0], dict) and \"error\" in results[0]) else 0,\n",
    "            \"raw_results\": results, \"answer\": answer, \"retrieval_method\": \"Text2Cypher\"\n",
    "        }\n",
    "\n",
    "# --- New Standalone Function to Explain Cypher ---\n",
    "def explain_cypher_in_natural_language(\n",
    "    cypher_query_string: str,\n",
    "    graph_schema_string: str,\n",
    "    openai_client: OpenAI,\n",
    "    model: str = \"gpt-3.5-turbo-0125\" # Or gpt-4o-mini\n",
    ") -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Uses an LLM to explain a Cypher query in natural language, given the graph schema.\n",
    "    \"\"\"\n",
    "    if not openai_client:\n",
    "        logging.error(\"OpenAI client not provided. Cannot explain Cypher.\")\n",
    "        return None\n",
    "    if not cypher_query_string:\n",
    "        logging.warning(\"Empty Cypher query string provided.\")\n",
    "        return \"No Cypher query to explain.\"\n",
    "    if \"Schema not available\" in graph_schema_string or \"Error fetching schema\" in graph_schema_string:\n",
    "        logging.warning(\"Graph schema is not available or contains errors. Explanation quality may be affected.\")\n",
    "\n",
    "    system_prompt = \"\"\"You are an expert at explaining Neo4j Cypher queries in simple, step-by-step natural language.\n",
    "Given the following graph schema and Cypher query, provide a clear explanation of what the query is doing.\n",
    "Describe:\n",
    "1. Which types of nodes (entities) the query starts by looking for.\n",
    "2. How it traverses relationships to find other related nodes.\n",
    "3. What conditions or filters are applied to these nodes or relationships.\n",
    "4. What information the query ultimately returns.\n",
    "Make the explanation easy for someone not deeply familiar with Cypher to understand.\n",
    "\"\"\"\n",
    "\n",
    "    user_prompt = f\"\"\"\n",
    "Graph Schema Overview:\n",
    "---\n",
    "{graph_schema_string}\n",
    "---\n",
    "\n",
    "Cypher Query to Explain:\n",
    "---\n",
    "{cypher_query_string}\n",
    "---\n",
    "\n",
    "Step-by-step Natural Language Explanation:\n",
    "\"\"\"\n",
    "    prompt_messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        completion = openai_client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=prompt_messages,\n",
    "            temperature=0.2\n",
    "        )\n",
    "        explanation = completion.choices[0].message.content.strip()\n",
    "        logging.info(f\"Successfully explained Cypher query:\\n{explanation}\")\n",
    "        return explanation\n",
    "    except Exception as e:\n",
    "        logging.error(f\"LLM call for Cypher explanation failed: {e}\")\n",
    "        return \"Could not generate an explanation for the Cypher query due to an error.\"\n",
    "\n",
    "# --- Instantiate and Test Text2CypherRetriever ---\n",
    "if 'neo4j_driver' in locals() and neo4j_driver and \\\n",
    "   'openai_client' in locals() and openai_client:\n",
    "\n",
    "    text2cypher_retriever = Text2CypherInvestmentRetriever(\n",
    "        neo4j_driver=neo4j_driver,\n",
    "        openai_client=openai_client\n",
    "    )\n",
    "    logging.info(f\"Schema for Text2Cypher (first 200 chars): \\n{text2cypher_retriever.schema[:200]}...\")\n",
    "\n",
    "    # Test Query 1 (AAPL News & Sector)\n",
    "    aapl_processed = any(comp.get('ticker') == 'AAPL' and not comp.get('error') for comp in detailed_companies_data) if 'detailed_companies_data' in locals() else False\n",
    "    if aapl_processed:\n",
    "        test_question_t2c_1 = \"What is the latest news sentiment for Apple (AAPL) and its current sector?\"\n",
    "        response_t2c_1 = text2cypher_retriever.query_kg(test_question_t2c_1)\n",
    "        print(\"\\n--- Test Query 1 (Text2Cypher) ---\")\n",
    "        print(f\"Question: {response_t2c_1['question']}\")\n",
    "        print(f\"Generated Cypher:\\n{response_t2c_1['cypher_query']}\")\n",
    "        print(f\"Answer: {response_t2c_1['answer']}\")\n",
    "        if response_t2c_1['cypher_query']: # Explain if Cypher was generated\n",
    "            print(\"\\n--- Cypher Explanation for Query 1 ---\")\n",
    "            explanation_q1 = explain_cypher_in_natural_language(response_t2c_1['cypher_query'], text2cypher_retriever.schema, openai_client)\n",
    "            print(explanation_q1)\n",
    "    else:\n",
    "        print(\"\\nSkipping Test Query 1 for Text2Cypher (AAPL data not processed/available).\")\n",
    "\n",
    "    # Test Query 2 (MSFT, ESG ETFs, Summary)\n",
    "    msft_processed = any(comp.get('ticker') == 'MSFT' and not comp.get('error') for comp in detailed_companies_data) if 'detailed_companies_data' in locals() else False\n",
    "    esgu_exists = 'etf_df' in locals() and 'ticker' in etf_df.columns and not etf_df[etf_df['ticker'] == 'ESGU'].empty\n",
    "    if msft_processed and esgu_exists:\n",
    "        test_question_t2c_2 = \"List ETFs that hold Microsoft (MSFT) and focus on ESG. Also, what is Microsoft's investment profile summary?\"\n",
    "        response_t2c_2 = text2cypher_retriever.query_kg(test_question_t2c_2)\n",
    "        print(\"\\n--- Test Query 2 (Text2Cypher) ---\")\n",
    "        print(f\"Question: {response_t2c_2['question']}\")\n",
    "        print(f\"Generated Cypher:\\n{response_t2c_2['cypher_query']}\")\n",
    "        print(f\"Answer: {response_t2c_2['answer']}\")\n",
    "        if response_t2c_2['cypher_query']:\n",
    "            print(\"\\n--- Cypher Explanation for Query 2 ---\")\n",
    "            explanation_q2 = explain_cypher_in_natural_language(response_t2c_2['cypher_query'], text2cypher_retriever.schema, openai_client)\n",
    "            print(explanation_q2)\n",
    "    else:\n",
    "        print(\"\\nSkipping Test Query 2 for Text2Cypher (MSFT or ESGU ETF data not available/processed, or etf_df lacks 'ticker' column).\")\n",
    "\n",
    "    # Test Query 3 (SPY & IT Companies Count)\n",
    "    spy_exists = 'etf_df' in locals() and 'ticker' in etf_df.columns and not etf_df[etf_df['ticker'] == 'SPY'].empty\n",
    "    if spy_exists:\n",
    "        test_question_t2c_3 = \"How many companies in the 'Information Technology' sector are held by the 'SPY' ETF in our sample data?\"\n",
    "        response_t2c_3 = text2cypher_retriever.query_kg(test_question_t2c_3)\n",
    "        print(\"\\n--- Test Query 3 (Text2Cypher) ---\")\n",
    "        print(f\"Question: {response_t2c_3['question']}\")\n",
    "        print(f\"Generated Cypher:\\n{response_t2c_3['cypher_query']}\")\n",
    "        print(f\"Answer: {response_t2c_3['answer']}\")\n",
    "        if response_t2c_3['cypher_query']:\n",
    "            print(\"\\n--- Cypher Explanation for Query 3 ---\")\n",
    "            explanation_q3 = explain_cypher_in_natural_language(response_t2c_3['cypher_query'], text2cypher_retriever.schema, openai_client)\n",
    "            print(explanation_q3)\n",
    "    else:\n",
    "        print(\"\\nSkipping Test Query 3 for Text2Cypher (SPY ETF data not available or etf_df lacks 'ticker' column).\")\n",
    "\n",
    "    # Test Query 4: Involving Investors\n",
    "    nvda_processed = any(comp.get('ticker') == 'NVDA' and not comp.get('error') for comp in detailed_companies_data) if 'detailed_companies_data' in locals() else False\n",
    "    ais_processed = any(comp.get('ticker') == 'AIS' and not comp.get('error') for comp in detailed_companies_data) if 'detailed_companies_data' in locals() else False\n",
    "    investor_data_exists = 'investor_df' in locals() and not investor_df.empty\n",
    "    if nvda_processed and ais_processed and investor_data_exists:\n",
    "        test_question_t2c_4 = \"Which investors have funded both NVIDIA (NVDA) and AI Startup Inc. (AIS)?\"\n",
    "        response_t2c_4 = text2cypher_retriever.query_kg(test_question_t2c_4)\n",
    "        print(\"\\n--- Test Query 4 (Text2Cypher - Common Investors) ---\")\n",
    "        print(f\"Question: {response_t2c_4['question']}\")\n",
    "        print(f\"Generated Cypher:\\n{response_t2c_4['cypher_query']}\")\n",
    "        print(f\"Answer: {response_t2c_4['answer']}\")\n",
    "        if response_t2c_4['cypher_query']:\n",
    "            print(\"\\n--- Cypher Explanation for Query 4 ---\")\n",
    "            explanation_q4 = explain_cypher_in_natural_language(response_t2c_4['cypher_query'], text2cypher_retriever.schema, openai_client)\n",
    "            print(explanation_q4)\n",
    "    else:\n",
    "        print(\"\\nSkipping Test Query 4 for Text2Cypher (NVDA, AIS, or investor data not available/processed).\")\n",
    "\n",
    "else:\n",
    "    logging.warning(\"Neo4j driver or OpenAI client not initialized. Skipping Text2CypherRetriever tests.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "GFVNp3yx4c5_",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from typing import List, Dict, Any, Optional\n",
    "\n",
    "class VectorSimilarityInvestmentRetriever:\n",
    "    def __init__(self, vector_store: Any, openai_client: OpenAI):\n",
    "        self.vector_store = vector_store\n",
    "        self.client = openai_client\n",
    "        self.answer_gen_model = \"gpt-4.1-nano\"\n",
    "\n",
    "    def retrieve_similar_nodes(self, query_text: str, k: int = 5, score_threshold: Optional[float] = None) -> List[Dict[str, Any]]:\n",
    "        if not self.vector_store:\n",
    "            logging.error(\"Neo4jVector store instance not provided. Cannot retrieve.\")\n",
    "            return []\n",
    "\n",
    "        logging.info(f\"Performing vector similarity search for: '{query_text}' (k={k})\")\n",
    "        try:\n",
    "            if score_threshold:\n",
    "                 results_with_scores = self.vector_store.similarity_search_with_score(\n",
    "                     query_text, k=k, score_threshold=score_threshold\n",
    "                 )\n",
    "            else:\n",
    "                 results_with_scores = self.vector_store.similarity_search_with_score(query_text, k=k)\n",
    "\n",
    "            processed_results = []\n",
    "            for doc, score in results_with_scores:\n",
    "                node_data = {\"similarity_score\": score, \"text_content\": doc.page_content}\n",
    "                node_data.update(doc.metadata)\n",
    "                processed_results.append(node_data)\n",
    "\n",
    "            logging.info(f\"Found {len(processed_results)} similar nodes via vector search.\")\n",
    "            return processed_results\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error during vector similarity search: {e}\")\n",
    "            return []\n",
    "\n",
    "    def synthesize_answer_from_vector_results(self, question: str, similar_nodes: List[Dict[str, Any]]) -> str:\n",
    "        if not self.client:\n",
    "            logging.warning(\"OpenAI client not initialized. Skipping answer synthesis.\")\n",
    "            return \"Could not synthesize answer: OpenAI client not available.\"\n",
    "\n",
    "        if not similar_nodes:\n",
    "            logging.warning(f\"No similar nodes found to synthesize answer for '{question}'.\")\n",
    "            return \"I couldn't find any information directly matching your semantic query.\"\n",
    "\n",
    "        context_items = []\n",
    "        for i, node in enumerate(similar_nodes[:3]):\n",
    "            item_desc = f\"Entity {i+1} (Similarity: {node.get('similarity_score', 0.0):.2f}):\\n\"\n",
    "            item_desc += f\"  Type: {node.get('_labels', ['Unknown'])}\\n\"\n",
    "            item_desc += f\"  Name/Identifier: {node.get('name', node.get('ticker', 'N/A'))}\\n\"\n",
    "            item_desc += f\"  Key Text: {node.get('text_content', 'N/A')[:250]}...\\n\"\n",
    "            if 'investmentProfileSummary' in node:\n",
    "                 item_desc += f\"  Investment Profile: {node['investmentProfileSummary'][:150]}...\\n\"\n",
    "            if 'investmentFocus' in node:\n",
    "                 item_desc += f\"  Investment Focus: {node['investmentFocus'][:150]}...\\n\"\n",
    "            context_items.append(item_desc)\n",
    "\n",
    "        context_str = \"\\n\".join(context_items)\n",
    "\n",
    "        prompt_messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a financial analyst assistant. Based on the user's question and the semantically similar entities retrieved from a knowledge graph, provide a concise and relevant natural language answer.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"\"\"\n",
    "Original Question: {question}\n",
    "\n",
    "Retrieved Semantically Similar Entities from Knowledge Graph:\n",
    "{context_str}\n",
    "\n",
    "Based on these entities, answer the original question.\n",
    "Answer:\n",
    "            \"\"\"}\n",
    "        ]\n",
    "\n",
    "        try:\n",
    "            completion = self.client.chat.completions.create(\n",
    "                model=self.answer_gen_model,\n",
    "                messages=prompt_messages,\n",
    "                temperature=0.3\n",
    "            )\n",
    "            answer = completion.choices[0].message.content.strip()\n",
    "            logging.info(f\"Synthesized answer for '{question}' from vector results: {answer}\")\n",
    "            return answer\n",
    "        except Exception as e:\n",
    "            logging.error(f\"LLM answer synthesis (vector) call failed: {e}\")\n",
    "            return \"There was an issue generating the final answer.\"\n",
    "\n",
    "    def query_kg_semantic(self, question: str, k: int = 3) -> Dict[str, Any]:\n",
    "        logging.info(f\"\\n--- Vector Similarity Processing Question: \\\"{question}\\\" ---\")\n",
    "        similar_nodes = self.retrieve_similar_nodes(question, k=k)\n",
    "        answer = self.synthesize_answer_from_vector_results(question, similar_nodes)\n",
    "\n",
    "        return {\n",
    "            \"question\": question,\n",
    "            \"retrieved_nodes_count\": len(similar_nodes),\n",
    "            \"raw_retrieved_nodes\": similar_nodes,\n",
    "            \"answer\": answer,\n",
    "            \"retrieval_method\": \"VectorSimilarity\"\n",
    "        }\n",
    "\n",
    "if 'vector_retriever_store' in locals() and vector_retriever_store and openai_client:\n",
    "    vector_retriever = VectorSimilarityInvestmentRetriever(\n",
    "        vector_store=vector_retriever_store,\n",
    "        openai_client=openai_client\n",
    "    )\n",
    "\n",
    "    test_question_vec_1 = \"Companies leading in AI-driven financial technology\"\n",
    "    response_vec_1 = vector_retriever.query_kg_semantic(test_question_vec_1, k=2)\n",
    "    print(\"\\n--- Test Query 1 (Vector Similarity) ---\")\n",
    "    print(f\"Question: {response_vec_1['question']}\")\n",
    "    print(f\"Answer: {response_vec_1['answer']}\")\n",
    "\n",
    "    test_question_vec_2 = \"ETFs that offer exposure to renewable energy markets\"\n",
    "    response_vec_2 = vector_retriever.query_kg_semantic(test_question_vec_2, k=2)\n",
    "    print(\"\\n--- Test Query 2 (Vector Similarity) ---\")\n",
    "    print(f\"Question: {response_vec_2['question']}\")\n",
    "    print(f\"Answer: {response_vec_2['answer']}\")\n",
    "else:\n",
    "    logging.warning(\"Neo4jVector store or OpenAI client not initialized. Skipping VectorSimilarityRetriever tests.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xStt8aFS4c6A",
   "metadata": {
    "id": "xStt8aFS4c6A"
   },
   "source": [
    "## Part 6: The Router Retriever - Strategically Selecting the Right Tool\n",
    "\n",
    "We've now developed two powerful, yet distinct, retrieval strategies for our Investment Knowledge Graph:\n",
    "1.  **Text2Cypher Retriever:** Excellent for precise, structured queries that translate well into specific graph traversals.\n",
    "2.  **Vector Similarity Retriever:** Ideal for conceptual, semantic, or exploratory queries where the exact entities or relationships might not be known upfront.\n",
    "\n",
    "In a sophisticated, production-grade AI system, how do you choose which retriever to use for a given user question? This is where a **Router Retriever** comes in.\n",
    "\n",
    "**What is a Router Retriever?**\n",
    "\n",
    "A Router Retriever is an intelligent component, often powered by an LLM itself or a set of heuristic rules, that analyzes an incoming natural language query and directs it to the most appropriate downstream retriever.\n",
    "\n",
    "**Key Responsibilities of a Router:**\n",
    "1.  **Query Analysis:** Understand the *intent* and *structure* of the user's question.\n",
    "2.  **Strategy Selection:** Based on the analysis, choose the optimal retriever (Text2Cypher, Vector Similarity, Hybrid).\n",
    "3.  **Reasoning (Optional but Recommended):** Log *why* a particular strategy was chosen.\n",
    "\n",
    "**Building a Router (Conceptual Approach for this Notebook):**\n",
    "\n",
    "Developing a full-fledged router is an advanced topic. For this notebook, we'll outline the conceptual approach:\n",
    "\n",
    "1.  **LLM as a Router:** Use an LLM with a prompt that includes the user's query and descriptions of available retrieval strategies, asking it to choose the best one.\n",
    "2.  **Heuristic/Rule-Based Routing:** Implement rules based on keywords or query structure.\n",
    "\n",
    "**The Engineering Value for Your Systems:**\n",
    "A well-designed router is critical for the performance, accuracy, and cost-effectiveness of a RAG system. It ensures the right tool is used for the job, leading to better results and more efficient resource utilization.\n",
    "\n",
    "While we won't implement a full router class here, understanding its role is key to architecting robust Graph RAG solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "plk_FwTN4c6B",
   "metadata": {
    "id": "plk_FwTN4c6B"
   },
   "source": [
    "## Part 7: Practical Analysis: Graph RAG vs. Vector RAG for Investment Insights\n",
    "\n",
    "Let's address a critical question: **When does the added complexity of a Knowledge Graph and Graph RAG provide a tangible advantage over a simpler Vector RAG system?**\n",
    "\n",
    "**Scenario 1: Multi-Hop Relational Queries**\n",
    "\n",
    "* **Question:** \"Which ETFs hold both Apple (AAPL) and Microsoft (MSFT) and have an ESG focus?\"\n",
    "* **Graph RAG (Text2Cypher) Advantage:** High precision due to explicit relationship traversal. Can guarantee the ETF holds *both* stocks and meets ESG criteria from graph properties.\n",
    "* **Traditional Vector RAG Challenge:** Lower precision, potential for false positives/negatives, difficulty in reliably verifying multi-condition relationships from text alone.\n",
    "* **Key Takeaway for Engineering Leaders:** For queries demanding high accuracy on multiple, interconnected criteria, Graph RAG's ability to traverse verified relationships is superior.\n",
    "\n",
    "**Scenario 2: Discovering Non-Obvious Connections & Network Effects**\n",
    "\n",
    "* **Question:** \"Are there common investors between NVIDIA (NVDA) and other companies in the 'Artificial intelligence' industry that have also received positive news sentiment recently?\"\n",
    "* **Graph RAG (Text2Cypher) Advantage:** Uncovers complex, indirect relationships and network effects by traversing multiple relationship types (investments, industry, news sentiment).\n",
    "* **Traditional Vector RAG Challenge:** Very difficult to reliably identify and verify such multi-hop, multi-conditional network patterns from document similarity alone.\n",
    "* **Key Takeaway for Product Managers:** Graph RAG can power features that provide unique \"connected insights\" – a strong differentiator for AI products.\n",
    "\n",
    "**Scenario 3: Conceptual Search Combined with Structured Filtering**\n",
    "\n",
    "* **Question:** \"Find me innovative healthcare companies with strong recent performance (e.g., high forward P/E) and a focus on 'personalized medicine'.\"\n",
    "* **Graph RAG (Hybrid: Vector Search + Text2Cypher) Advantage:** Combines exploratory power of semantic search (for \"innovative personalized medicine\") with precise structured graph filtering (for financial metrics).\n",
    "* **Traditional Vector RAG Challenge:** May struggle to accurately apply quantitative filters directly during vector search.\n",
    "* **Key Takeaway for CTOs:** A hybrid Graph RAG architecture offers versatility, allowing the system to choose the best path – semantic, structural, or both.\n",
    "\n",
    "**Conclusion of Comparison:**\n",
    "\n",
    "The investment in building a Knowledge Graph pays off when your application needs to:\n",
    "* Answer questions dependent on **complex relationships and multiple hops**.\n",
    "* Ensure **high precision and verifiability** of these relationships.\n",
    "* Combine **semantic understanding with structured filtering** seamlessly.\n",
    "* Uncover **network effects and non-obvious connections**.\n",
    "\n",
    "For many advanced business intelligence and investment analysis use cases, the depth and precision offered by Graph RAG can provide a significant competitive advantage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "UkRxZDeH4c6C",
   "metadata": {
    "id": "UkRxZDeH4c6C"
   },
   "source": [
    "## Conclusion & Next Steps: Building ROI-Driven AI with Graph RAG\n",
    "\n",
    "We've journeyed from foundational company and investment data to a sophisticated Investment Knowledge Graph, enriched by LLMs and queryable through advanced RAG techniques.\n",
    "\n",
    "**Key Takeaways for Your AI Strategy:**\n",
    "\n",
    "1.  **Knowledge Graphs Unlock Deeper Insights:** Structuring data around entities and relationships allows for complex, multi-faceted questions, crucial in domains like finance.\n",
    "2.  **LLMs as Graph Augmenters & Interpreters:** LLMs enrich KGs (sentiment, summaries) and enable natural language access (Text2Cypher).\n",
    "3.  **Graph RAG for Precision and Context:** For queries dependent on verified relationships, Graph RAG offers superior precision.\n",
    "4.  **Vector Search on Graphs for Exploration:** Integrating vector indexes provides powerful semantic search for conceptual queries.\n",
    "5.  **Strategic Query Routing is Key:** A router selecting the best retrieval strategy is crucial for performance, accuracy, and cost.\n",
    "6.  **ROI in Advanced RAG:** Translates into faster insights, discovery of non-obvious patterns, powerful AI product features, and better decision support.\n",
    "\n",
    "**From Proof-of-Concept to Production:**\n",
    "\n",
    "As an AI engineering consultant, I help organizations navigate from AI experiments to robust, scalable, ROI-driven production systems. The concepts here – KG construction, LLM enrichment, advanced RAG, and query routing – are foundational for high-impact AI solutions.\n",
    "\n",
    "**Potential Next Steps for Your Organization:**\n",
    "\n",
    "* **Identify High-Value Use Cases:** Where do complex relationships and contextual insights offer the biggest leverage?\n",
    "* **Data Audit & Modeling:** Assess your data sources for KG potential.\n",
    "* **Pilot Project:** Start with a focused pilot to demonstrate Graph RAG value.\n",
    "* **Performance & Scalability Planning:** Consider optimized ingestion, query tuning, and MLOps for LLM pipelines.\n",
    "* **Measure Business Impact:** Define KPIs to track outcomes from improved insights.\n",
    "\n",
    "The path to advanced AI involves innovative techniques and sound engineering. Knowledge Graphs, thoughtfully applied with modern LLMs, are a significant step in transforming data into strategic intelligence.\n",
    "\n",
    "---\n",
    "\n",
    "Thank you for working through this notebook. I hope it has provided valuable insights into the power of Advanced Graph RAG. If you're looking to explore how these concepts can be tailored and implemented to solve your specific business challenges, I'm here to help.\n",
    "\n",
    "*Saumil Srivastava*\n",
    "*AI Engineering Consultant*\n",
    "*[Book a consultation](https://www.saumilsrivastava.ai/book-consultation)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MN2cjFgEuXGw",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
